{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/massaro8/cnn-pytorch-example/blob/main/Introduzione_cnn_pytorch_empty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b1b8d1",
      "metadata": {
        "id": "e8b1b8d1"
      },
      "source": [
        "\n",
        "# Introduzione alle CNN con PyTorch\n",
        "\n",
        "\n",
        "## Obiettivi didattici:  \n",
        "1. Capire cos'√® una **convoluzione** (filtri, kernel, stride, padding) e perch√© √® utile con le immagini.  \n",
        "2. Comprendere il **pooling** e il concetto di invarianza locale.  \n",
        "3. Costruire, addestrare, valutare e salvare una piccola **CNN** in PyTorch sul dataset **MNIST**.  \n",
        "4. Usare strumenti pratici: scelta automatica del **device** (CPU/GPU), barra di progresso con `tqdm`, funzione di **accuratezza**.  \n",
        "5. Migliorare la rete con **regolarizzazione** (dropout, L2/weight decay), **Batch Normalization** e **scheduler** del learning rate.  \n",
        "\n",
        "## Architettura CNN\n",
        "\n",
        "![Alt text](https://media.datacamp.com/cms/ad_4nxct55fjxboktz5ezpyzmmkc28dy6tk3s_djp9uljfjwigsm4oagqnrvbr-edpro2ggylzl4odhtbc3xapxf-y527snl-i_noynj1uteapbm_erw-hijzkvaqmt9oiap8__pp083.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fab680f7",
      "metadata": {
        "id": "fab680f7"
      },
      "source": [
        "\n",
        "## Prerequisiti e setup\n",
        "\n",
        "Esegui la cella seguente per importare le librerie. In **Colab** molte sono gi√† presenti.  \n",
        "\n",
        "```bash\n",
        "\n",
        "# Installazione pacchetti principali\n",
        "pip install --upgrade torch torchvision torchaudio tqdm matplotlib scikit-learn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade torch torchvision torchaudio tqdm matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "gYUAGBW7NzoK"
      },
      "id": "gYUAGBW7NzoK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab015b7",
      "metadata": {
        "id": "aab015b7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Librerie standard Python\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "\n",
        "# Libreria per calcolo numerico e manipolazione di array multidimensionali\n",
        "import numpy as np\n",
        "\n",
        "# Libreria per grafici e visualizzazioni (curve di training, immagini, confusion matrix, ecc.)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Libreria principale per il Deep Learning\n",
        "import torch\n",
        "\n",
        "\n",
        "# Moduli principali di PyTorch\n",
        "from torch import nn,optim\n",
        "\n",
        "\n",
        "# Sotto-modulo con funzioni matematiche/attivazioni da usare direttamente (es. F.relu, F.softmax)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Modulo per la gestione dei dataset\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "\n",
        "# Libreria PyTorch per la Computer Vision (dataset, trasformazioni e modelli pre-addestrati)\n",
        "import torchvision\n",
        "from torchvision import datasets,transforms\n",
        "\n",
        "\n",
        "# Libreria per barre di avanzamento eleganti e automatiche\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Stampa le versioni di PyTorch e Torchvision (utile per debug o compatibilit√†)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "\n",
        "\n",
        "# Funzione per fissare i semi casuali (reproducibilit√†)\n",
        "# Garantisce che ogni esecuzione produca gli stessi risultati\n",
        "def set_seed(seed: int = 42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Richiama la funzione di fissaggio del seed casuale\n",
        "set_seed(seed = 42)\n",
        "\n",
        "# Se √® disponibile una GPU CUDA, la utilizza; altrimenti usa la CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Stampa quale dispositivo √® stato selezionato\n",
        "print(\"Device\",device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156f09d2",
      "metadata": {
        "id": "156f09d2"
      },
      "source": [
        "# üß† Basi delle CNN ‚Äî Convoluzione (Filtri, Stride, Padding, Feature Map)\n",
        "\n",
        "Le Convolutional Neural Networks (CNN) sono reti progettate per analizzare immagini e dati spaziali, sfruttando il concetto di convoluzione: un‚Äôoperazione matematica che combina un‚Äôimmagine d‚Äôingresso con un piccolo filtro (o kernel) per estrarre pattern locali come bordi, angoli o texture.\n",
        "\n",
        "## üîπ 1. Filtri (Kernels)\n",
        "\n",
        "Un filtro (o kernel) √® una piccola matrice di pesi (es. 3√ó3 o 5√ó5) che scorre sull‚Äôimmagine originale (input).\n",
        "A ogni posizione, il filtro e la finestra dell‚Äôimmagine vengono moltiplicati elemento per elemento e poi sommati ‚Üí ottenendo un singolo numero nel risultato.\n",
        "\n",
        "**Questo processo si ripete su tutta l‚Äôimmagine, generando una feature map.**\n",
        "\n",
        "üìò Ogni filtro impara a riconoscere un tipo specifico di pattern:\n",
        "\n",
        "* Un filtro pu√≤ riconoscere i bordi orizzontali,\n",
        "\n",
        "* Un altro pu√≤ riconoscere i bordi verticali,\n",
        "\n",
        "* Altri possono individuare curve o texture pi√π complesse.\n",
        "\n",
        "üìä Se applichi 8 filtri diversi, otterrai 8 feature map, ciascuna con informazioni differenti.\n",
        "\n",
        "üì∑ Esempio visivo: come 2 filtri 3√ó3 generano 2 feature map da un‚Äôimmagine 7√ó7√ó3\n",
        "\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*IGfVdsOnPl6MIwMO4V33IQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 2. Stride\n",
        "\n",
        "Lo stride indica di quanti pixel il filtro avanza a ogni passo (sia orizzontale che verticale).\n",
        "\n",
        "* Stride = 1 ‚Üí il filtro si muove di 1 pixel per volta ‚Üí output grande (massimo dettaglio).\n",
        "\n",
        "* Stride = 2 ‚Üí il filtro si muove di 2 pixel per volta ‚Üí output pi√π piccolo (riduzione dimensionale).\n",
        "\n",
        "üì∑ Esempio visivo: kernel 2√ó2 con stride=2 su immagine 5√ó5\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*DYi9mm55THoCH_-vqWXd7A.png)"
      ],
      "metadata": {
        "id": "FGwqzXGk3xXn"
      },
      "id": "FGwqzXGk3xXn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 3. Padding\n",
        "\n",
        "Il padding aggiunge dei ‚Äúpixel fittizi‚Äù (spesso zero) intorno all‚Äôimmagine di input per controllare la dimensione dell‚Äôoutput e preservare le informazioni ai bordi.\n",
        "\n",
        "* Senza padding: l‚Äôimmagine si ‚Äúrimpicciolisce‚Äù dopo ogni convoluzione.\n",
        "\n",
        "* Con padding: possiamo mantenere la stessa dimensione dell‚Äôimmagine originale.\n",
        "\n",
        "üì∑ Esempio visivo: aggiunta di padding=1 a un‚Äôimmagine 3√ó3\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*mf0nv_ajmLn7-TdmdnxFlQ.png)"
      ],
      "metadata": {
        "id": "UAhjW0VZ36Zl"
      },
      "id": "UAhjW0VZ36Zl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 4. Feature Map\n",
        "\n",
        "Il risultato della convoluzione √® una nuova immagine detta feature map o activation map.\n",
        "Ogni feature map rappresenta come un determinato filtro ‚Äúvede‚Äù l‚Äôimmagine originale.\n",
        "\n",
        "üì∑ Esempio visivo: diverse feature map prodotte da filtri differenti su immagini di cifre MNIST\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*11AOltIb4osdgt8Dtc98Kw.png)"
      ],
      "metadata": {
        "id": "w2YYdlvP4D_5"
      },
      "id": "w2YYdlvP4D_5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7882274",
      "metadata": {
        "id": "e7882274"
      },
      "outputs": [],
      "source": [
        "# Creiamo una piccola immagine 2D di dimensione 6x6 inizialmente tutta a zero (pixel neri)\n",
        "img = np.zeros((6, 6), dtype=np.float32)\n",
        "\n",
        "# Impostiamo a 1.0 (pixel bianchi) un quadrato 2x2 al centro: righe 2-3 e colonne 2-3 (lo slice finale √® esclusivo)\n",
        "# Cos√¨ otteniamo un pattern semplice su cui \"vedere\" l'effetto della convoluzione\n",
        "img[2:4,2:4] = 1.0\n",
        "\n",
        "# Definiamo un kernel (filtro) 3x3 di tipo  per il gradiente ORIZZONTALE o VERTICALE\n",
        "# ATTENZIONE: questi pesi corrispondono al  Gx (o a un suo segno invertito),\n",
        "# cio√® rilevano il GRADIENTE LUNGO X ‚Üí evidenziano BORDI VERTICALI (cambiamenti sinistra‚Üîdestra).\n",
        "# Per bordi verticali si usano valori con colonne [+1, 0, -1] (righe pesate 1,2,1).\n",
        "# Intuizione: la colonna di sinistra √® positiva (+1, +2, +1), quella di destra negativa (-1, -2, -1).\n",
        "# Se a sinistra della finestra ho intensit√† pi√π alte che a destra, la somma sar√† positiva (bordi \"in un verso\");\n",
        "# invertendo il segno si ribalta la direzione del contrasto (chiaro‚Üíscuro vs scuro‚Üíchiaro).\n",
        "kernel = np.array([[ 1,  0, -1],\n",
        "                   [ 2,  0, -2],\n",
        "                   [ 1,  0, -1]], dtype=np.float32)\n",
        "\n",
        "\n",
        "def conv2d_naive(image, kernel, stride=1, padding=0):\n",
        "    \"\"\"\n",
        "    Implementazione \"naive\" della convoluzione 2D usando NumPy.\n",
        "    NOTA: come nella maggior parte dei framework deep learning,\n",
        "    qui eseguiamo in realt√† una CORRELAZIONE (non ruotiamo il kernel di 180¬∞).\n",
        "    Questo √® ci√≤ che fanno anche PyTorch e TensorFlow nelle conv2d.\n",
        "    \"\"\"\n",
        "    # Applichiamo il padding (se richiesto) aggiungendo bordi di zeri attorno all'immagine.\n",
        "    # Il padding √® utile per: (1) non \"perdere\" informazione ai bordi; (2) controllare la dimensione dell'output\n",
        "\n",
        "    if padding > 0:\n",
        "      image = np.pad(\n",
        "          image,\n",
        "          ((padding,padding),(padding,padding)),# (alto,basso),(sinistra)\n",
        "          mode= \"constant\",\n",
        "          constant_values=0\n",
        "          )\n",
        "\n",
        "      # H, W: altezza e larghezza dell'immagine (DOPO il padding, se applicato)\n",
        "    H,W = image.shape\n",
        "\n",
        "    # kH, kW: altezza e larghezza del kernel\n",
        "    kH,kW = kernel.shape\n",
        "\n",
        "    # Calcoliamo la dimensione spaziale dell'output in base a formula classica:\n",
        "    # out = floor((in - kernel) / stride) + 1   (qui \"in\" √® gi√† H o W post-padding)\n",
        "    #print(f\"H = {H} & kh = {kH} & W = {W} & kW ={kW}\")\n",
        "\n",
        "    outH = (H - kH) // stride + 1\n",
        "    outW = (W - kW) // stride + 1\n",
        "\n",
        "    # Inizializziamo la mappa di attivazione (feature map) di output a zeri\n",
        "    out = np.zeros((outH,outW),dtype=np.float32)\n",
        "\n",
        "    # Doppio ciclo per scorrere il kernel su ogni posizione valida dell'immagine\n",
        "    for i in range(outH):\n",
        "      for j in range(outW):\n",
        "        patch = image[i*stride:i*stride + kH,\n",
        "                      j*stride:j*stride + kW]\n",
        "\n",
        "        out[i,j] = np.sum(patch * kernel)\n",
        "\n",
        "\n",
        "    # Ritorniamo la feature map risultante\n",
        "    return out\n",
        "\n",
        "# Convoluzione SENZA padding e con stride=1\n",
        "# L'output si restringe (perch√© il kernel \"non entra\"0  sui bordi): da 6x6 con kernel 3x3 ottieni 4x4\n",
        "out_no_pad = conv2d_naive(img,kernel,stride=1,padding=0)\n",
        "\n",
        "# Convoluzione CON padding=1 e stride=1\n",
        "# Il padding \"allarga\" l'immagine a 8x8 prima della conv; con kernel 3x3 e stride=1 l'output torna 6x6.\n",
        "# (Manteniamo la stessa dimensione spaziale dell'input \"originale\": comportamento detto \"same\")\n",
        "out_pad_1 = conv2d_naive(img,kernel,stride=1,padding=1)\n",
        "\n",
        "# Convoluzione CON padding=1 ma stride=2\n",
        "# Lo stride=2 \"salta\" una cella a ogni passo ‚Üí output pi√π piccolo (downsampling): da 6x6 ottieni 3x3\n",
        "out_pad_2 = conv2d_naive(img,kernel,stride=2,padding=1)\n",
        "\n",
        "# Stampiamo le forme per verificare visivamente gli effetti di padding e stride\n",
        "print(\"Forma input\",img.shape)\n",
        "print(\"Senza padding,stride=1\",out_no_pad.shape)\n",
        "print(\"Padding=1,stride=1\",out_pad_1.shape)\n",
        "print(\"Padding=1, stride=2\",out_pad_2.shape)\n",
        "\n",
        "# Visualizziamo l'immagine di partenza (6x6) in scala di grigi\n",
        "plt.figure()\n",
        "plt.imshow(img,cmap ='gray')\n",
        "plt.title(\"Immagine iniziale (6x6)\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualizziamo l'output della convoluzione con padding=1 e stride=1 (stessa dimensione dell'input)\n",
        "# Qui ci aspettiamo valori alti in corrispondenza di bordi VERTICALI del quadrato bianco\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(out_no_pad,cmap ='gray')\n",
        "plt.title(\"Immagine senza padding\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(out_pad_1,cmap ='gray')\n",
        "plt.title(\"Immagine padding 1\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(out_pad_2,cmap ='gray')\n",
        "plt.title(\"Immagine padding 2\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# ==========================\n",
        "# 1. Creiamo immagine e kernel\n",
        "# ==========================\n",
        "img = np.zeros((6, 6), dtype=np.float32)\n",
        "img[2:4, 2:4] = 1.0  # piccolo quadrato bianco\n",
        "\n",
        "# kernel = np.array([[ 1,  0, -1],\n",
        "#                    [ 2,  0, -2],\n",
        "#                    [ 1,  0, -1]], dtype=np.float32)\n",
        "\n",
        "kernel = np.array([[ 1, 2, 1],\n",
        "                   [ 0,  0, 0],\n",
        "                   [ -1,  -2, -1]], dtype=np.float32)\n",
        "\n",
        "stride = 1\n",
        "padding = 1\n",
        "\n",
        "# Applichiamo padding\n",
        "padded_img = np.pad(img, ((padding, padding), (padding, padding)), mode='constant')\n",
        "H, W = padded_img.shape\n",
        "kH, kW = kernel.shape\n",
        "outH = (H - kH)//stride + 1\n",
        "outW = (W - kW)//stride + 1\n",
        "out = np.zeros((outH, outW), dtype=np.float32)\n",
        "\n",
        "# ==========================\n",
        "# 2. Setup figura animata\n",
        "# ==========================\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax1.set_title(\"Input + Kernel (con valori)\")\n",
        "ax2.set_title(\"Feature Map (valori aggiornati)\")\n",
        "\n",
        "im1 = ax1.imshow(padded_img, cmap='gray', vmin=0, vmax=1)\n",
        "im2 = ax2.imshow(out, cmap='gray', vmin=-4, vmax=4)\n",
        "\n",
        "# Rettangolo che rappresenta il kernel\n",
        "rect = plt.Rectangle((0, 0), kW-1, kH-1, edgecolor='red', facecolor='none', lw=2)\n",
        "ax1.add_patch(rect)\n",
        "\n",
        "for ax in (ax1, ax2):\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Testo dinamico\n",
        "text_kernel = fig.text(0.05, 0.02, \"\", fontsize=12, color='darkred')\n",
        "text_value = fig.text(0.55, 0.02, \"\", fontsize=12, color='darkblue')\n",
        "\n",
        "# Numeri della feature map\n",
        "texts_out = [[ax2.text(j, i, \"\", ha=\"center\", va=\"center\", color=\"white\", fontsize=10)\n",
        "              for j in range(outW)] for i in range(outH)]\n",
        "\n",
        "# Numeri del kernel che verranno visualizzati dentro il quadrato rosso\n",
        "texts_kernel = [[ax1.text(0, 0, \"\", ha=\"center\", va=\"center\", color=\"yellow\", fontsize=10)\n",
        "                 for _ in range(kW)] for _ in range(kH)]\n",
        "\n",
        "# ==========================\n",
        "# 3. Funzione di aggiornamento\n",
        "# ==========================\n",
        "def update(frame):\n",
        "    i = frame // outW\n",
        "    j = frame % outW\n",
        "\n",
        "    # estrai patch corrente\n",
        "    patch = padded_img[i*stride : i*stride + kH,\n",
        "                       j*stride : j*stride + kW]\n",
        "\n",
        "    value = np.sum(patch * kernel)\n",
        "    out[i, j] = value\n",
        "\n",
        "    # aggiorna rettangolo\n",
        "    rect.set_xy((j, i))\n",
        "    im2.set_data(out)\n",
        "\n",
        "    # aggiorna testi dinamici\n",
        "    text_kernel.set_text(f\"üß≠ Calcolando output[{i},{j}]\")\n",
        "    text_value.set_text(f\"üìà Œ£(patch √ó kernel) = {value:.2f}\")\n",
        "\n",
        "    # aggiorna numeri sulla feature map\n",
        "    for y in range(outH):\n",
        "        for x in range(outW):\n",
        "            texts_out[y][x].set_text(f\"{out[y,x]:.1f}\" if out[y,x] != 0 else \"\")\n",
        "\n",
        "    # aggiorna numeri del kernel (centrati nella posizione corrente)\n",
        "    for ki in range(kH):\n",
        "        for kj in range(kW):\n",
        "            x_pos = j + kj\n",
        "            y_pos = i + ki\n",
        "            texts_kernel[ki][kj].set_position((x_pos, y_pos))\n",
        "            texts_kernel[ki][kj].set_text(f\"{int(kernel[ki, kj])}\")\n",
        "\n",
        "    return [im1, im2, rect, text_kernel, text_value] + sum(texts_out, []) + sum(texts_kernel, [])\n",
        "\n",
        "# ==========================\n",
        "# 4. Animazione\n",
        "# ==========================\n",
        "ani = animation.FuncAnimation(\n",
        "    fig, update, frames=outH*outW, interval=1000, blit=False, repeat=True\n",
        ")\n",
        "\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "wf4HZiz-EBrZ"
      },
      "id": "wf4HZiz-EBrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e69a7ff6",
      "metadata": {
        "id": "e69a7ff6"
      },
      "source": [
        "# üß≠ Recap: cosa abbiamo fatto finora\n",
        "\n",
        "Nel blocco precedente abbiamo:\n",
        "\n",
        "1. **Creato un‚Äôimmagine artificiale 6√ó6**, quasi tutta nera, con un piccolo quadrato bianco al centro.\n",
        "‚Üí Serve per osservare visivamente come i filtri rispondono ai bordi.\n",
        "\n",
        "2. **Applicato una convoluzione 2D con un kernel(3√ó3)** che misura le variazioni di intensit√† orizzontali.\n",
        "\n",
        "Il nostro kernel:\n",
        "\n",
        "* colonne di sinistra positive (+1, +2, +1)\n",
        "\n",
        "* colonne di destra negative (‚àí1, ‚àí2, ‚àí1)\n",
        "‚Üí rileva bordi verticali, cio√® transizioni nero ‚Üî bianco.\n",
        "\n",
        "Visto l‚Äôeffetto di padding e stride:\n",
        "\n",
        "* Padding=0 ‚Üí l‚Äôimmagine ‚Äúsi restringe‚Äù\n",
        "\n",
        "* Padding=1 ‚Üí mantiene la stessa dimensione\n",
        "\n",
        "* Stride=2 ‚Üí dimezza l‚Äôoutput (downsampling)\n",
        "\n",
        "Visualizzato l‚Äôoutput della convoluzione:\n",
        "\n",
        "* Valori positivi (bianco) ‚Üí bordo da scuro ‚Üí chiaro\n",
        "\n",
        "* Valori negativi (nero) ‚Üí bordo da chiaro ‚Üí scuro\n",
        "\n",
        "* Valori vicini a 0 ‚Üí regioni uniformi senza bordi\n",
        "\n",
        "üëâ In pratica, l‚Äôoutput rappresenta dove il filtro ‚Äúvede‚Äù un bordo verticale.\n",
        "Ogni valore della feature map √® la risposta del filtro in quella posizione.\n",
        "\n",
        "## üß© Passaggio successivo ‚Äî Pooling\n",
        "\n",
        "Ora applichiamo un pooling sull‚Äôoutput della convoluzione per:\n",
        "\n",
        "1. ridurre la dimensione della feature map,\n",
        "\n",
        "2. preservare le caratteristiche principali,\n",
        "\n",
        "3. rendere il modello pi√π robusto a piccoli spostamenti.\n",
        "\n",
        "Esistono due tipi principali di pooling:\n",
        "\n",
        "* **Max Pooling:** prende il valore massimo di ogni blocco (conserva le attivazioni pi√π forti)\n",
        "\n",
        "* **Average Pooling:** prende la media di ogni blocco (effetto di ‚Äúlisciatura‚Äù)\n",
        "\n",
        "Visivamente ‚¨áÔ∏è\n",
        "\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:640/format:webp/1*sIf_zGTXgvTEDkpeP8jvxg.jpeg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1],[2],[3]])\n",
        "print(f\"Tensore iniziale = {x} & shape = {x.shape}\")\n",
        "\n",
        "x_squeezed = torch.squeeze(x)\n",
        "print(f\"x_squezzed =  {x_squeezed} & shape = {x_squeezed.shape}\")\n",
        "\n",
        "x_unsqeezed = torch.unsqueeze(x_squeezed,dim=1)\n",
        "print(f\"x_unsqueezed = {x_unsqeezed} & shape = {x_unsqeezed.shape}\")\n",
        "\n",
        "x2 = torch.arange(1,9)\n",
        "print(f\"x2 = {x2} & shape = {x2.shape}\")\n",
        "\n",
        "x2_view = x2.view(2,4)\n",
        "print(f\"x2_view = {x2_view} & shape = {x2_view.shape}\")\n",
        "\n",
        "x2_reshape = x2.reshape(4,2)\n",
        "print(f\"x2_reshape = {x2_reshape} & shape = {x2_reshape.shape}\")\n",
        "\n",
        "x3 = torch.randn(2,3,4)\n",
        "print(f\"x3 = {x3} & shape = {x3.shape}\")\n",
        "\n",
        "x3_T = x3.transpose(0,1)\n",
        "print(f\"x3_t = {x3_T} & shape = {x3_T.shape}\")"
      ],
      "metadata": {
        "id": "Uy8nmuZzn1nX"
      },
      "id": "Uy8nmuZzn1nX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd76ec0",
      "metadata": {
        "id": "edd76ec0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convertiamo l'output della convoluzione (out_pad_1, un array NumPy 6x6) in un TENSORE PyTorch.\n",
        "# Le reti convoluzionali in PyTorch si aspettano input con forma:\n",
        "# (N, C, H, W)\n",
        "# dove:\n",
        "#   N = numero di esempi (batch size)\n",
        "#   C = numero di canali (es. 1 per immagini in scala di grigi, 3 per RGB)\n",
        "#   H = altezza (height)\n",
        "#   W = larghezza (width)\n",
        "\n",
        "# out_pad_1 ha shape (6, 6) ‚Üí solo H e W.\n",
        "# Per renderlo compatibile, aggiungiamo:\n",
        "# - una dimensione per il batch (N=1)\n",
        "# - una per il canale (C=1)\n",
        "# Risultato: (1, 1, 6, 6)\n",
        "feature_map = torch.tensor(out_pad_1,dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "print(f\"feaure_map = {feature_map.shape}\")\n",
        "\n",
        "# Definiamo i due tipi di pooling:\n",
        "# MaxPool2d: seleziona il valore massimo in ogni finestra (kernel 2x2)\n",
        "# AvgPool2d: calcola la media dei valori nella stessa finestra\n",
        "# Lo stride=2 significa che ogni finestra \"salta\" di 2 pixel ‚Üí dimezza H e W (6x6 ‚Üí 3x3)\n",
        "maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "avgpool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "# Applichiamo il pooling alla feature map\n",
        "max_pooled = maxpool(feature_map)\n",
        "avg_pooled = avgpool(feature_map)\n",
        "\n",
        "# Stampiamo le forme per verificare la riduzione spaziale\n",
        "print(f\"Max pooling = {max_pooled.shape}\")\n",
        "print(f\"Avg pooling = {avg_pooled.shape}\")\n",
        "\n",
        "# Visualizziamo i risultati a confronto:\n",
        "# - Feature map originale (output della convoluzione)\n",
        "# - Max pooling (3x3): mantiene solo i valori massimi locali\n",
        "# - Average pooling (3x3): calcola la media locale, smussando i dettagli\n",
        "fig, axes = plt.subplots(1,3,figsize = (10,3))\n",
        "\n",
        "axes[0].imshow(out_pad_1,cmap= 'gray')\n",
        "axes[0].set_title(\"Feature map originale\")\n",
        "\n",
        "axes[1].imshow(max_pooled.squeeze(), cmap='gray')\n",
        "axes[1].set_title(\"Max pooling\")\n",
        "\n",
        "axes[2].imshow(avg_pooled.squeeze(),cmap='gray')\n",
        "axes[2].set_title(\"Avg Pooling\")\n",
        "\n",
        "for ax in axes:\n",
        "  ax.axis('off')\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Rimuoviamo gli assi per una visualizzazione pi√π pulita\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Cosa osserviamo\n",
        "\n",
        "* Dopo il **Max Pooling**, l‚Äôimmagine √® pi√π piccola (6√ó6 ‚Üí 3√ó3) ma i bordi principali sono ancora ben evidenti.\n",
        "\n",
        "* Dopo **l‚ÄôAverage Pooling**, le variazioni si ‚Äúammorbidiscono‚Äù: le intensit√† estreme vengono mediate.\n",
        "\n",
        "‚úÖ Conclusione\n",
        "\n",
        "Il pooling serve per:\n",
        "\n",
        "* ridurre la quantit√† di dati da elaborare nei layer successivi,\n",
        "\n",
        "* mantenere solo le informazioni pi√π significative,\n",
        "\n",
        "* garantire invarianza a traslazioni locali (es. un bordo spostato di 1 pixel non cambia molto l‚Äôattivazione del blocco).\n",
        "\n",
        "In una CNN reale, dopo ogni convoluzione si alternano tipicamente:\n",
        "\n",
        "**Conv ‚Üí ReLU ‚Üí Pooling**\n",
        "\n",
        "per costruire progressivamente rappresentazioni pi√π compatte ma pi√π ricche di significato."
      ],
      "metadata": {
        "id": "KYmpFwgxjA0O"
      },
      "id": "KYmpFwgxjA0O"
    },
    {
      "cell_type": "markdown",
      "id": "93c9a290",
      "metadata": {
        "id": "93c9a290"
      },
      "source": [
        "\n",
        "## La tua prima CNN (MNIST)\n",
        "\n",
        "Costruiremo una piccola CNN per **MNIST** (cifre 28√ó28 in scala di grigi).  \n",
        "**Pipeline:**\n",
        "1. **Transforms**: conversione a tensore e **normalizzazione** (media=0.1307, std=0.3081).  \n",
        "2. **Dataset & DataLoader** per train/val/test.  \n",
        "3. **Modello**: due blocchi conv ‚Üí flatten ‚Üí fully connected.  \n",
        "4. **Training loop** con `CrossEntropyLoss` + `Adam`.  \n",
        "5. **Accuratezza** e barra `tqdm` per il progresso.  \n",
        "6. **Grafici** di loss e accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beaf6715",
      "metadata": {
        "id": "beaf6715"
      },
      "outputs": [],
      "source": [
        "# 3.1 Trasformazioni (normalizzazione consigliata per MNIST)\n",
        "# Compose concatena pi√π trasformazioni da applicare in sequenza a ogni immagine.\n",
        "# ToTensor: converte un'immagine PIL/array [0..255] in un tensore float [0..1] con shape (C,H,W).\n",
        "# Normalize: per ogni canale applica (x - mean) / std. Per MNIST (grayscale) c'√® un solo canale.\n",
        "#            mean=0.1307 e std=0.3081 sono statistiche standard del dataset MNIST.\n",
        "transform_mnist = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,),(0.3081,))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3.2 Dataset (con fallback)\n",
        "# Directory locale dove scaricare o cercare i file del dataset.\n",
        "data_root = './data'\n",
        "\n",
        "# Proviamo a caricare/scaricare MNIST:\n",
        "# - train=True: split di addestramento (60k immagini)\n",
        "# - train=False: split di test (10k immagini)\n",
        "# - transform=transform_mnist: applica le trasformazioni definite sopra\n",
        "# - download=True: scarica se non √® gi√† presente nella cartella data_root\n",
        "try:\n",
        "  train_full = datasets.MNIST(root=data_root,train=True,transform=transform_mnist,download=True)\n",
        "  test_ds = datasets.MNIST(root=data_root,train=False,transform=transform_mnist,download=True)\n",
        "except Exception as e:\n",
        "  print(f\"impossibile scaricare MNIST\",str(e))\n",
        "\n",
        "# 3.3 Split train/val\n",
        "# Percentuale del training set da usare come validazione (10%).\n",
        "val_ratio = 0.1\n",
        "val_size = int(len(train_full) * val_ratio)\n",
        "train_size = len(train_full) - val_size\n",
        "train_ds,val_ds = random_split(train_full,[train_size,val_size])\n",
        "len(train_full),len(train_ds),len(val_ds),len(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3.4 DataLoader\n",
        "# Dimensione dei batch (128 √® un buon compromesso per MNIST; puoi aumentarla con GPU pi√π capienti).\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=2,\n",
        "                          pin_memory=True if device.type == 'cuda' else False\n",
        "                          )\n",
        "val_loader = DataLoader(val_ds,batch_size=batch_size, shuffle=True,num_workers=2,\n",
        "                        pin_memory=True if device.type == 'cuda' else False\n",
        "                        )\n",
        "test_loader = DataLoader(test_ds,batch_size=batch_size, shuffle=True,num_workers=2,\n",
        "                        pin_memory=True if device.type == 'cuda' else False\n",
        "                        )\n"
      ],
      "metadata": {
        "id": "I2PaRhlW12xW"
      },
      "id": "I2PaRhlW12xW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1bfc66",
      "metadata": {
        "id": "3c1bfc66"
      },
      "outputs": [],
      "source": [
        "# 3.5 Visualizziamo alcune immagini con le etichette\n",
        "def show_batch(dl, n=5):\n",
        "    # Stampa la lunghezza del DataLoader, ovvero il numero totale di batch.\n",
        "    # Esempio: se il dataset ha 54.000 immagini e batch_size=128 ‚Üí ci saranno circa 422 batch.\n",
        "    print(len(dl))\n",
        "\n",
        "    # next(iter(dl)) estrae il PRIMO batch dal DataLoader.\n",
        "    # xb = immagini (batch di tensori)\n",
        "    # yb = etichette (batch di target numerici)\n",
        "    xb, yb = next(iter(dl))\n",
        "\n",
        "    # Stampa le shape dei tensori per capire come sono strutturati.\n",
        "    print(xb.shape)  # (batch_size, canali, altezza, larghezza)\n",
        "    print(yb.shape)  # (batch_size,)\n",
        "\n",
        "    #print(xb)  # (batch_size, canali, altezza, larghezza)\n",
        "    #print(yb)  # (batch_size,)\n",
        "\n",
        "    # Seleziona solo i primi n campioni del batch (es. 5 immagini)\n",
        "    xb, yb = xb[:n], yb[:n]\n",
        "\n",
        "    # Sposta i dati su CPU e li converte in array NumPy per poterli plottare con Matplotlib.\n",
        "    xb = xb.cpu().numpy()\n",
        "\n",
        "    # Crea una nuova figura.\n",
        "    plt.figure()\n",
        "\n",
        "    # cols = numero di colonne nel layout dei subplot (una riga con n immagini)\n",
        "    cols = n\n",
        "    for i in range(n):\n",
        "        # Crea una subplot 1√ón, posizione i+1\n",
        "        plt.subplot(1, cols, i+1)\n",
        "\n",
        "        # xb[i][0]: seleziona la i-esima immagine e il primo canale (0, perch√© √® in bianco e nero)\n",
        "        plt.imshow(xb[i][0], cmap='gray')\n",
        "\n",
        "        # Mostra il numero della classe come titolo\n",
        "        plt.title(f\"Label: {int(yb[i])}\")\n",
        "\n",
        "        # Nasconde gli assi per una visualizzazione pi√π pulita\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Mostra tutte le immagini insieme\n",
        "    plt.show()\n",
        "\n",
        "# Esegui la funzione per visualizzare 5 immagini dal training set\n",
        "show_batch(train_loader, n=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f826f6b5",
      "metadata": {
        "id": "f826f6b5"
      },
      "outputs": [],
      "source": [
        "# 3.6 Definiamo la CNN\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self,dropout:0,use_batchnorm=False):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    c1,c2 = 5,10\n",
        "\n",
        "    layers = []\n",
        "    #(28,28)\n",
        "    layers += [nn.Conv2d(1,c1,kernel_size=3,padding=1),\n",
        "               nn.ReLU(inplace=True)]\n",
        "\n",
        "    if use_batchnorm:\n",
        "      layers += [nn.BatchNorm2d(c1)]\n",
        "\n",
        "    layers += [nn.MaxPool2d(2,2)] # 28 -> 14\n",
        "\n",
        "    layers += [nn.Conv2d(c1,c2,kernel_size=3,padding=1), # (14,14)\n",
        "               nn.ReLU(inplace=True)]\n",
        "\n",
        "    if use_batchnorm:\n",
        "      layers += [nn.BatchNorm2d(c2)]\n",
        "\n",
        "    layers += [nn.MaxPool2d(2,2)] # 14 -> 7\n",
        "\n",
        "    self.features = nn.Sequential(*layers)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "    self.classifier = nn.Linear(7 * 7 *c2 ,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.dropout(x)\n",
        "    x= self.classifier(x)\n",
        "    return x\n",
        "\n",
        "def count_params(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "model = CNN(dropout=0,use_batchnorm=False).to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print(\"Parametri addestrabili\",count_params(model))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833dd572",
      "metadata": {
        "id": "833dd572"
      },
      "outputs": [],
      "source": [
        "# Calcola l'accuratezza a partire dai logits (uscita del modello) e dalle label vere y\n",
        "def accuracy(logits,y):\n",
        "  preds = logits.argmax(dim=1)\n",
        "  \"\"\"\n",
        "  logits = [0.9,0.1,0,0,0,0]\n",
        "  preds = 0\n",
        "\n",
        "  tensor([0])\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  return (preds == y).float().mean().item()\n",
        "\n",
        "\n",
        "# Esegue UNA epoca di training (forward + backward + update pesi) e restituisce loss/accuracy medie sull'epoca\n",
        "def train_epochs(model,loader,optimizer,criterior):\n",
        "  model.train()\n",
        "  running_loss,running_acc = 0.0,0.0\n",
        "\n",
        "  for xb,yb in tqdm(loader,desc='Train',leave=False):\n",
        "\n",
        "    xb,yb = xb.to(device),yb.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model(xb)\n",
        "\n",
        "    loss = criterior(logits,yb)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    running_acc += accuracy(logits,yb) * xb.size(0)\n",
        "\n",
        "  n = len(loader.dataset)\n",
        "\n",
        "  return running_loss / n , running_acc / n\n",
        "\n",
        "# Valutazione (validazione/test): NESSUN gradiente, NESSUN update pesi\n",
        "@torch.no_grad()\n",
        "def evaluate(model,loader,criterior):\n",
        "  model.eval()\n",
        "  running_loss, running_acc = 0.0,0.0\n",
        "\n",
        "  for xb,yb in tqdm(loader,desc='val',leave= False):\n",
        "    xb,yb = xb.to(device),yb.to(device)\n",
        "    logits = model(xb)\n",
        "    loss = criterior(logits,yb)\n",
        "\n",
        "    running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    running_acc += accuracy(logits,yb) * xb.size(0)\n",
        "\n",
        "  n = len(loader.dataset)\n",
        "\n",
        "  return running_loss / n , running_acc / n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7a5786",
      "metadata": {
        "id": "9a7a5786"
      },
      "outputs": [],
      "source": [
        "# 3.8 Addestriamo!\n",
        "epochs = 3\n",
        "criterior = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=2,gamma = 0.5)\n",
        "\n",
        "history = {\"train_loss\":[],\"val_loss\":[],\"train_acc\":[],\"val_acc\":[]}\n",
        "\n",
        "for epoch in range(1,epochs + 1):\n",
        "  print(f\"Epochs {epoch}/{epochs}\")\n",
        "  tl,ta = train_epochs(model,train_loader,optimizer,criterior)\n",
        "  vl,va = evaluate(model,val_loader,criterior)\n",
        "  scheduler.step()\n",
        "\n",
        "  history[\"train_loss\"].append(tl)\n",
        "  history[\"train_acc\"].append(ta)\n",
        "  history[\"val_loss\"].append(vl)\n",
        "  history[\"val_acc\"].append(va)\n",
        "\n",
        "  print(f\"loss_train = {tl:.4f} acc_train = {ta:.4f} | loss_val = {vl:.4f} acc_val = {va:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c3a071",
      "metadata": {
        "id": "60c3a071"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3.9 Grafici delle curve di apprendimento (matplotlib; un grafico per figura; nessuno stile colore specifico)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae98826f",
      "metadata": {
        "id": "ae98826f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3.10 Valutazione sul test set + alcune predizioni\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563b591b",
      "metadata": {
        "id": "563b591b"
      },
      "source": [
        "\n",
        "## 4. Regolarizzazione e miglioramenti\n",
        "\n",
        "**Perch√© servono?** Per ridurre **overfitting** e rendere l'addestramento pi√π stabile.\n",
        "\n",
        "- **Dropout:** azzera casualmente alcune attivazioni durante il training (es. `p=0.3`).  \n",
        "- **Weight decay (L2):** penalizza pesi grandi tramite l'ottimizzatore (es. `weight_decay=1e-4`).  \n",
        "- **Batch Normalization:** normalizza le attivazioni per velocizzare/stabilizzare il training.  \n",
        "- **Scheduler LR:** regola il learning rate durante l'addestramento (es. `StepLR`, `OneCycleLR`).\n",
        "\n",
        "Sotto attiviamo **dropout + batch norm** e facciamo un breve training dimostrativo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaea0e26",
      "metadata": {
        "id": "eaea0e26"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d86bdb",
      "metadata": {
        "id": "26d86bdb"
      },
      "source": [
        "\n",
        "## 4bis. Confusion Matrix ed Early Stopping\n",
        "\n",
        "In questa sezione impariamo due strumenti fondamentali per migliorare l‚Äôanalisi e la robustezza del modello:\n",
        "\n",
        "1. **Confusion Matrix (Matrice di confusione)**  \n",
        "   Serve per capire **quali classi il modello confonde** tra loro.  \n",
        "   Ogni cella mostra quante volte una classe reale √® stata predetta come un‚Äôaltra classe.\n",
        "\n",
        "   - La diagonale rappresenta le **predizioni corrette**.  \n",
        "   - Le celle fuori diagonale mostrano gli **errori di classificazione**.\n",
        "\n",
        "2. **Early Stopping (Interruzione anticipata)**  \n",
        "   Serve a fermare l‚Äôaddestramento **prima** che il modello inizi a sovradattarsi (*overfitting*).  \n",
        "   Se la **loss di validazione** non migliora dopo un certo numero di epoche (pazienza), si interrompe il training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ed0c8d",
      "metadata": {
        "id": "e4ed0c8d"
      },
      "outputs": [],
      "source": [
        "# Importiamo due utility da scikit-learn:\n",
        "# - confusion_matrix: calcola la matrice di confusione a partire da etichette vere e predette\n",
        "# - ConfusionMatrixDisplay: oggetto comodo per visualizzare la matrice con Matplotlib\n",
        "\n",
        "\n",
        "# --- Calcolo e visualizzazione della matrice di confusione sul TEST set ---\n",
        "\n",
        "                                         # Mostriamo la figura\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ef6e2f",
      "metadata": {
        "id": "b9ef6e2f"
      },
      "outputs": [],
      "source": [
        "# Early Stopping semplice: si interrompe se la loss di validazione non migliora per 'patience' epoche consecutive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e155aaf",
      "metadata": {
        "id": "9e155aaf"
      },
      "source": [
        "\n",
        "### Visualizzazione delle curve di Early Stopping\n",
        "\n",
        "Nei grafici seguenti vediamo come si comportano **loss** e **accuratezza** nel tempo:  \n",
        "- le curve blu rappresentano l‚Äôandamento del **training**;  \n",
        "- le curve arancioni rappresentano l‚Äô**insieme di validazione**;  \n",
        "- il **punto rosso** indica il momento in cui l‚Äôearly stopping ha interrotto l‚Äôaddestramento.\n",
        "\n",
        "Osservando i grafici puoi capire quando il modello ha smesso di migliorare sulla validazione, pur continuando a migliorare (overfitting) sul training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a69fb58",
      "metadata": {
        "id": "5a69fb58"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Troviamo l'epoca di stop (ultima epoca effettiva eseguita)\n",
        "stopped_epoch = len(hist_es[\"val_loss\"]) - 1\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist_es[\"train_loss\"], label=\"loss_train\")\n",
        "plt.plot(hist_es[\"val_loss\"], label=\"loss_val\")\n",
        "plt.scatter(stopped_epoch, hist_es[\"val_loss\"][stopped_epoch], color=\"red\", label=\"stop\")\n",
        "plt.legend(); plt.xlabel(\"epoca\"); plt.ylabel(\"loss\")\n",
        "plt.title(\"Early Stopping - Andamento della loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist_es[\"train_acc\"], label=\"acc_train\")\n",
        "plt.plot(hist_es[\"val_acc\"], label=\"acc_val\")\n",
        "plt.scatter(stopped_epoch, hist_es[\"val_acc\"][stopped_epoch], color=\"red\", label=\"stop\")\n",
        "plt.legend(); plt.xlabel(\"epoca\"); plt.ylabel(\"accuratezza\")\n",
        "plt.title(\"Early Stopping - Andamento dell'accuratezza\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f34deaa8",
      "metadata": {
        "id": "f34deaa8"
      },
      "source": [
        "\n",
        "## 5. Salvataggio e caricamento del modello\n",
        "\n",
        "Usa `state_dict` per salvare solo i pesi:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec44e718",
      "metadata": {
        "id": "ec44e718"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb62a42",
      "metadata": {
        "id": "fcb62a42"
      },
      "source": [
        "\n",
        "## 6. (Opzionale) Assaggio di transfer learning (CIFAR-10)\n",
        "\n",
        "**Idea:** partire da un modello pre-addestrato su un grande dataset (es. ImageNet) e adattarlo al tuo compito.\n",
        "\n",
        "Useremo **ResNet18** da `torchvision.models`. Se non √® possibile scaricare il dataset, ricorriamo a `FakeData` (3√ó32√ó32, ridimensionate a 224√ó224).  \n",
        "√à solo un assaggio: per risultati solidi servono pi√π epoche e un setup pi√π curato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f480eb8b",
      "metadata": {
        "id": "f480eb8b"
      },
      "outputs": [],
      "source": [
        "# Importiamo la ResNet18 e i pesi pre-addestrati disponibili in torchvision\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 1Ô∏è‚É£ Definizione delle trasformazioni\n",
        "# ----------------------------\n",
        "\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# 2Ô∏è‚É£ Caricamento dataset CIFAR-10 (con fallback)\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3Ô∏è‚É£ Creazione DataLoader\n",
        "# ----------------------------\n",
        "# batch_size=64 ‚Üí 64 immagini per batch\n",
        "# shuffle=True nel training ‚Üí mescola i dati a ogni epoca per migliorare la generalizzazione\n",
        "# pin_memory=True (solo su GPU): migliora le prestazioni delle copie CPU‚ÜíGPU\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4Ô∏è‚É£ Caricamento modello pre-addestrato\n",
        "# ----------------------------\n",
        "# Otteniamo i pesi predefiniti di ResNet18 (pretrained su ImageNet)\n",
        "\n",
        "\n",
        "# Carichiamo la rete con quei pesi\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 5Ô∏è‚É£ Congelamento dei layer di feature extraction\n",
        "# ----------------------------\n",
        "# In questo modo NON aggiorniamo i pesi delle convoluzioni gi√† apprese su ImageNet\n",
        "# (evitiamo di distruggere conoscenze generali come rilevatori di bordi, texture, forme)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6Ô∏è‚É£ Sostituzione della testa di classificazione\n",
        "# ----------------------------\n",
        "# ResNet18 pre-addestrata ha un layer finale (fc) con 1000 output (per ImageNet)\n",
        "# Lo sostituiamo con un layer fully connected per 10 classi (CIFAR-10)\n",
        "\n",
        "\n",
        "# Spostiamo tutto il modello sul device (CPU o GPU)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 7Ô∏è‚É£ Ottimizzatore e loss function\n",
        "# ----------------------------\n",
        "# Addestriamo SOLO la nuova testa (gli altri layer restano congelati)\n",
        "\n",
        "# Loss per classificazione multi-classe\n",
        "\n",
        "# ----------------------------\n",
        "# 8Ô∏è‚É£ Mini addestramento dimostrativo\n",
        "# ----------------------------\n",
        "# Solo 1 epoca per mostrare come funziona il transfer learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classi di CIFAR-10\n",
        "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def predict_and_show(model, loader, n=5):\n",
        "    \"\"\"\n",
        "    Mostra n immagini del loader con la predizione del modello.\n",
        "    \"\"\"\n",
        "    model.eval()  # disattiva dropout e batchnorm in modalit√† training\n",
        "    xb, yb = next(iter(loader))          # prende il primo batch\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    with torch.no_grad():                # no gradient ‚Üí pi√π veloce, meno memoria\n",
        "        preds = model(xb).argmax(1)      # indice della classe con logit pi√π alto\n",
        "\n",
        "    xb = xb.cpu().permute(0, 2, 3, 1)    # da (N,C,H,W) ‚Üí (N,H,W,C) per Matplotlib\n",
        "    xb = xb * torch.tensor((0.229, 0.224, 0.225)) + torch.tensor((0.485, 0.456, 0.406))\n",
        "    xb = torch.clamp(xb, 0, 1)           # de-normalizzazione per visualizzare correttamente\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i in range(n):\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(xb[i])\n",
        "        plt.title(f\"True: {cifar_classes[yb[i]]}\\nPred: {cifar_classes[preds[i]]}\",\n",
        "                  fontsize=10)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# üîç Eseguiamo la funzione di predizione\n",
        "predict_and_show(backbone, cifar_test_loader, n=5)\n"
      ],
      "metadata": {
        "id": "wzNg4cHfulwi"
      },
      "id": "wzNg4cHfulwi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}