{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8b1b8d1",
      "metadata": {
        "id": "e8b1b8d1"
      },
      "source": [
        "\n",
        "# Introduzione alle CNN con PyTorch\n",
        "\n",
        "\n",
        "## Obiettivi didattici:  \n",
        "1. Capire cos'√® una **convoluzione** (filtri, kernel, stride, padding) e perch√© √® utile con le immagini.  \n",
        "2. Comprendere il **pooling** e il concetto di invarianza locale.  \n",
        "3. Costruire, addestrare, valutare e salvare una piccola **CNN** in PyTorch sul dataset **MNIST**.  \n",
        "4. Usare strumenti pratici: scelta automatica del **device** (CPU/GPU), barra di progresso con `tqdm`, funzione di **accuratezza**.  \n",
        "5. Migliorare la rete con **regolarizzazione** (dropout, L2/weight decay), **Batch Normalization** e **scheduler** del learning rate.  \n",
        "6. (Opzionale) Intuire cos‚Äô√® il **transfer learning** con un modello pre-addestrato.\n",
        "\n",
        "## Architettura CNN\n",
        "\n",
        "![Alt text](https://media.datacamp.com/cms/ad_4nxct55fjxboktz5ezpyzmmkc28dy6tk3s_djp9uljfjwigsm4oagqnrvbr-edpro2ggylzl4odhtbc3xapxf-y527snl-i_noynj1uteapbm_erw-hijzkvaqmt9oiap8__pp083.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fab680f7",
      "metadata": {
        "id": "fab680f7"
      },
      "source": [
        "\n",
        "## Prerequisiti e setup\n",
        "\n",
        "Esegui la cella seguente per importare le librerie. In **Colab** molte sono gi√† presenti.  \n",
        "Su macchina locale, ti consigliamo un ambiente virtuale e:\n",
        "```bash\n",
        "# (Facoltativo) Nuovo ambiente virtuale\n",
        "# python -m venv .venv && source .venv/bin/activate  # Linux/Mac\n",
        "# .venv\\Scripts\\activate                            # Windows\n",
        "\n",
        "# Installazione pacchetti principali\n",
        "pip install --upgrade torch torchvision torchaudio tqdm matplotlib scikit-learn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade torch torchvision torchaudio tqdm matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "gYUAGBW7NzoK"
      },
      "id": "gYUAGBW7NzoK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab015b7",
      "metadata": {
        "id": "aab015b7"
      },
      "outputs": [],
      "source": [
        "# Se hai bisogno di installare le librerie direttamente dal notebook, decommenta la riga seguente.\n",
        "# √à utile in Google Colab o in un nuovo ambiente di sviluppo.\n",
        "# %pip install --upgrade torch torchvision torchaudio tqdm matplotlib scikit-learn\n",
        "\n",
        "\n",
        "# Librerie standard Python\n",
        "import os          # Gestione del file system (path, directory, variabili d'ambiente, ecc.)\n",
        "import random      # Generazione di numeri casuali nativi di Python (utile per la riproducibilit√†)\n",
        "import math        # Funzioni matematiche di base (es. sqrt, ceil, floor, log, ecc.)\n",
        "import time        # Misura o gestione dei tempi di esecuzione del codice (utile per benchmark)\n",
        "\n",
        "\n",
        "# Libreria per calcolo numerico e manipolazione di array multidimensionali\n",
        "import numpy as np  # NumPy √® spesso usato insieme a PyTorch per analisi e conversioni (tensor <-> array)\n",
        "\n",
        "\n",
        "# Libreria per grafici e visualizzazioni (curve di training, immagini, confusion matrix, ecc.)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Libreria principale per il Deep Learning\n",
        "import torch  # PyTorch: gestisce tensori, GPU, e calcolo automatico delle derivate (autograd)\n",
        "\n",
        "\n",
        "# Moduli principali di PyTorch\n",
        "from torch import nn, optim\n",
        "# nn  ‚Üí contiene layer predefiniti (Linear, Conv2d, ReLU, Dropout, ecc.) per costruire reti neurali\n",
        "# optim ‚Üí contiene algoritmi di ottimizzazione (Adam, SGD, RMSprop, ecc.) per aggiornare i pesi\n",
        "\n",
        "\n",
        "# Sotto-modulo con funzioni matematiche/attivazioni da usare direttamente (es. F.relu, F.softmax)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Modulo per la gestione dei dataset\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "# DataLoader ‚Üí crea batch di dati per l'addestramento\n",
        "# random_split ‚Üí divide un dataset in parti (es. train/validation)\n",
        "# Subset ‚Üí permette di selezionare sottoinsiemi specifici di un dataset\n",
        "\n",
        "\n",
        "# Libreria PyTorch per la Computer Vision (dataset, trasformazioni e modelli pre-addestrati)\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "# datasets  ‚Üí contiene dataset gi√† pronti (MNIST, CIFAR10, ImageNet, ecc.)\n",
        "# transforms ‚Üí contiene trasformazioni (es. ToTensor, Normalize, Resize) per preprocessare immagini\n",
        "\n",
        "\n",
        "# Libreria per barre di avanzamento eleganti e automatiche\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Stampa le versioni di PyTorch e Torchvision (utile per debug o compatibilit√†)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "\n",
        "\n",
        "# Funzione per fissare i semi casuali (reproducibilit√†)\n",
        "# Garantisce che ogni esecuzione produca gli stessi risultati\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)                      # Fissa la casualit√† del modulo random di Python\n",
        "    np.random.seed(seed)                   # Fissa la casualit√† di NumPy\n",
        "    torch.manual_seed(seed)                # Fissa la casualit√† di PyTorch (CPU)\n",
        "    torch.cuda.manual_seed_all(seed)       # Fissa la casualit√† di PyTorch per tutte le GPU\n",
        "    torch.backends.cudnn.deterministic = True  # Imposta le operazioni cuDNN come deterministiche\n",
        "    torch.backends.cudnn.benchmark = False     # Disattiva l‚Äôottimizzazione dinamica di cuDNN (serve per coerenza nei risultati)\n",
        "\n",
        "\n",
        "# Richiama la funzione di fissaggio del seme casuale\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "# Se √® disponibile una GPU CUDA, la utilizza; altrimenti usa la CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Stampa quale dispositivo √® stato selezionato\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156f09d2",
      "metadata": {
        "id": "156f09d2"
      },
      "source": [
        "# üß† Basi delle CNN ‚Äî Convoluzione (Filtri, Stride, Padding, Feature Map)\n",
        "\n",
        "Le Convolutional Neural Networks (CNN) sono reti progettate per analizzare immagini e dati spaziali, sfruttando il concetto di convoluzione: un‚Äôoperazione matematica che combina un‚Äôimmagine d‚Äôingresso con un piccolo filtro (o kernel) per estrarre pattern locali come bordi, angoli o texture.\n",
        "\n",
        "## üîπ 1. Filtri (Kernels)\n",
        "\n",
        "Un filtro (o kernel) √® una piccola matrice di pesi (es. 3√ó3 o 5√ó5) che scorre sull‚Äôimmagine originale (input).\n",
        "A ogni posizione, il filtro e la finestra dell‚Äôimmagine vengono moltiplicati elemento per elemento e poi sommati ‚Üí ottenendo un singolo numero nel risultato.\n",
        "\n",
        "**Questo processo si ripete su tutta l‚Äôimmagine, generando una feature map.**\n",
        "\n",
        "üìò Ogni filtro impara a riconoscere un tipo specifico di pattern:\n",
        "\n",
        "* Un filtro pu√≤ riconoscere i bordi orizzontali,\n",
        "\n",
        "* Un altro pu√≤ riconoscere i bordi verticali,\n",
        "\n",
        "* Altri possono individuare curve o texture pi√π complesse.\n",
        "\n",
        "üìä Se applichi 8 filtri diversi, otterrai 8 feature map, ciascuna con informazioni differenti.\n",
        "\n",
        "üì∑ Esempio visivo: come 2 filtri 3√ó3 generano 2 feature map da un‚Äôimmagine 7√ó7√ó3\n",
        "\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*IGfVdsOnPl6MIwMO4V33IQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 2. Stride\n",
        "\n",
        "Lo stride indica di quanti pixel il filtro avanza a ogni passo (sia orizzontale che verticale).\n",
        "\n",
        "* Stride = 1 ‚Üí il filtro si muove di 1 pixel per volta ‚Üí output grande (massimo dettaglio).\n",
        "\n",
        "* Stride = 2 ‚Üí il filtro si muove di 2 pixel per volta ‚Üí output pi√π piccolo (riduzione dimensionale).\n",
        "\n",
        "üì∑ Esempio visivo: kernel 2√ó2 con stride=2 su immagine 5√ó5\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*DYi9mm55THoCH_-vqWXd7A.png)"
      ],
      "metadata": {
        "id": "FGwqzXGk3xXn"
      },
      "id": "FGwqzXGk3xXn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 3. Padding\n",
        "\n",
        "Il padding aggiunge dei ‚Äúpixel fittizi‚Äù (spesso zero) intorno all‚Äôimmagine di input per controllare la dimensione dell‚Äôoutput e preservare le informazioni ai bordi.\n",
        "\n",
        "* Senza padding: l‚Äôimmagine si ‚Äúrimpicciolisce‚Äù dopo ogni convoluzione.\n",
        "\n",
        "* Con padding: possiamo mantenere la stessa dimensione dell‚Äôimmagine originale.\n",
        "\n",
        "üì∑ Esempio visivo: aggiunta di padding=1 a un‚Äôimmagine 3√ó3\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*mf0nv_ajmLn7-TdmdnxFlQ.png)"
      ],
      "metadata": {
        "id": "UAhjW0VZ36Zl"
      },
      "id": "UAhjW0VZ36Zl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ 4. Feature Map\n",
        "\n",
        "Il risultato della convoluzione √® una nuova immagine detta feature map o activation map.\n",
        "Ogni feature map rappresenta come un determinato filtro ‚Äúvede‚Äù l‚Äôimmagine originale.\n",
        "\n",
        "üì∑ Esempio visivo: diverse feature map prodotte da filtri differenti su immagini di cifre MNIST\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*11AOltIb4osdgt8Dtc98Kw.png)"
      ],
      "metadata": {
        "id": "w2YYdlvP4D_5"
      },
      "id": "w2YYdlvP4D_5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7882274",
      "metadata": {
        "id": "e7882274"
      },
      "outputs": [],
      "source": [
        "# Creiamo una piccola immagine 2D di dimensione 6x6 inizialmente tutta a zero (pixel neri)\n",
        "img = np.zeros((6, 6), dtype=np.float32)\n",
        "\n",
        "# Impostiamo a 1.0 (pixel bianchi) un quadrato 2x2 al centro: righe 2-3 e colonne 2-3 (lo slice finale √® esclusivo)\n",
        "# Cos√¨ otteniamo un pattern semplice su cui \"vedere\" l'effetto della convoluzione\n",
        "img[2:4, 2:4] = 1.0  # piccolo quadrato bianco\n",
        "\n",
        "# Definiamo un kernel (filtro) 3x3 di tipo \"Sobel\" per il gradiente ORIZZONTALE o VERTICALE?\n",
        "# ATTENZIONE: questi pesi corrispondono al Sobel Gx (o a un suo segno invertito),\n",
        "# cio√® rilevano il GRADIENTE LUNGO X ‚Üí evidenziano BORDI VERTICALI (cambiamenti sinistra‚Üîdestra).\n",
        "# Per bordi verticali si usano valori con colonne [+1, 0, -1] (righe pesate 1,2,1).\n",
        "# Intuizione: la colonna di sinistra √® positiva (+1, +2, +1), quella di destra negativa (-1, -2, -1).\n",
        "# Se a sinistra della finestra ho intensit√† pi√π alte che a destra, la somma sar√† positiva (bordi \"in un verso\");\n",
        "# invertendo il segno si ribalta la direzione del contrasto (chiaro‚Üíscuro vs scuro‚Üíchiaro).\n",
        "kernel = np.array([[ 1,  0, -1],\n",
        "                   [ 2,  0, -2],\n",
        "                   [ 1,  0, -1]], dtype=np.float32)\n",
        "\n",
        "# Se volessi invece un kernel per BORDI ORIZZONTALI (gradiente lungo Y), useresti il Sobel Gy:\n",
        "# kernel = np.array([[ 1,  2,  1],\n",
        "#                    [ 0,  0,  0],\n",
        "#                    [-1, -2, -1]], dtype=np.float32)\n",
        "\n",
        "def conv2d_naive(image, kernel, stride=1, padding=0):\n",
        "    \"\"\"\n",
        "    Implementazione \"naive\" della convoluzione 2D usando NumPy.\n",
        "    NOTA: come nella maggior parte dei framework deep learning,\n",
        "    qui eseguiamo in realt√† una CORRELAZIONE (non ruotiamo il kernel di 180¬∞).\n",
        "    Questo √® ci√≤ che fanno anche PyTorch e TensorFlow nelle conv2d.\n",
        "    \"\"\"\n",
        "    # Applichiamo il padding (se richiesto) aggiungendo bordi di zeri attorno all'immagine.\n",
        "    # Il padding √® utile per: (1) non \"perdere\" informazione ai bordi; (2) controllare la dimensione dell'output.\n",
        "    if padding > 0:\n",
        "        image = np.pad(\n",
        "            image,\n",
        "            ((padding, padding), (padding, padding)),  # pad su (alto,basso) e (sinistra,destra)\n",
        "            mode='constant',\n",
        "            constant_values=0\n",
        "        )\n",
        "\n",
        "    # H, W: altezza e larghezza dell'immagine (DOPO il padding, se applicato)\n",
        "    H, W = image.shape\n",
        "\n",
        "    # kH, kW: altezza e larghezza del kernel\n",
        "    kH, kW = kernel.shape\n",
        "\n",
        "    # Calcoliamo la dimensione spaziale dell'output in base a formula classica:\n",
        "    # out = floor((in - kernel) / stride) + 1   (qui \"in\" √® gi√† H o W post-padding)\n",
        "    outH = (H - kH) // stride + 1\n",
        "    outW = (W - kW) // stride + 1\n",
        "\n",
        "    # Inizializziamo la mappa di attivazione (feature map) di output a zeri\n",
        "    out = np.zeros((outH, outW), dtype=np.float32)\n",
        "\n",
        "    # Doppio ciclo per scorrere il kernel su ogni posizione valida dell'immagine\n",
        "    for i in range(outH):           # indice verticale dell'output\n",
        "        for j in range(outW):       # indice orizzontale dell'output\n",
        "            # Estraggo il \"patch\" (la finestrella) dell'immagine sotto il kernel\n",
        "            # La top-left del patch √® (i*stride, j*stride) e la finestra √® grande kH x kW\n",
        "            patch = image[i*stride : i*stride + kH,\n",
        "                          j*stride : j*stride + kW]\n",
        "\n",
        "            # Prodotto elemento per elemento tra patch e kernel e somma ‚Üí singolo numero in out[i, j]\n",
        "            # Questo valore √® alto quando il pattern \"assomiglia\" alle pesature del kernel (es. bordo verticale)\n",
        "            out[i, j] = np.sum(patch * kernel)\n",
        "\n",
        "    # Ritorniamo la feature map risultante\n",
        "    return out\n",
        "\n",
        "# Convoluzione SENZA padding e con stride=1\n",
        "# L'output si restringe (perch√© il kernel \"non entra\" sui bordi): da 6x6 con kernel 3x3 ottieni 4x4\n",
        "out_no_pad = conv2d_naive(img, kernel, stride=1, padding=0)\n",
        "\n",
        "# Convoluzione CON padding=1 e stride=1\n",
        "# Il padding \"allarga\" l'immagine a 8x8 prima della conv; con kernel 3x3 e stride=1 l'output torna 6x6.\n",
        "# (Manteniamo la stessa dimensione spaziale dell'input \"originale\": comportamento detto \"same\")\n",
        "out_pad_1 = conv2d_naive(img, kernel, stride=1, padding=1)\n",
        "\n",
        "# Convoluzione CON padding=1 ma stride=2\n",
        "# Lo stride=2 \"salta\" una cella a ogni passo ‚Üí output pi√π piccolo (downsampling): da 6x6 ottieni 3x3\n",
        "out_stride_2 = conv2d_naive(img, kernel, stride=2, padding=1)\n",
        "\n",
        "# Stampiamo le forme per verificare visivamente gli effetti di padding e stride\n",
        "print(\"Forma input:\", img.shape)\n",
        "print(\"Senza padding, stride=1 ‚Üí\", out_no_pad.shape)     # atteso: (4, 4)\n",
        "print(\"Padding=1, stride=1 ‚Üí\", out_pad_1.shape, \"(mantiene dimensione)\")  # atteso: (6, 6)\n",
        "print(\"Padding=1, stride=2 ‚Üí\", out_stride_2.shape, \"(uscita pi√π piccola)\") # atteso: (3, 3)\n",
        "\n",
        "# Visualizziamo l'immagine di partenza (6x6) in scala di grigi\n",
        "plt.figure()\n",
        "plt.imshow(img, cmap='gray')     # 'gray' mostra 0=nero, 1=bianco\n",
        "plt.title(\"Immagine giocattolo (6x6)\")\n",
        "plt.axis('off')                  # nascondiamo assi per pulizia\n",
        "plt.show()\n",
        "\n",
        "# Visualizziamo l'output della convoluzione con padding=1 e stride=1 (stessa dimensione dell'input)\n",
        "# Qui ci aspettiamo valori alti in corrispondenza di bordi VERTICALI del quadrato bianco\n",
        "plt.figure()\n",
        "plt.imshow(out_pad_1, cmap='gray')\n",
        "plt.title(\"Output convoluzione (pad=1, stride=1)\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# ==========================\n",
        "# 1. Creiamo immagine e kernel\n",
        "# ==========================\n",
        "img = np.zeros((6, 6), dtype=np.float32)\n",
        "img[2:4, 2:4] = 1.0  # piccolo quadrato bianco\n",
        "\n",
        "kernel = np.array([[ 1,  0, -1],\n",
        "                   [ 2,  0, -2],\n",
        "                   [ 1,  0, -1]], dtype=np.float32)\n",
        "\n",
        "stride = 1\n",
        "padding = 1\n",
        "\n",
        "# Applichiamo padding\n",
        "padded_img = np.pad(img, ((padding, padding), (padding, padding)), mode='constant')\n",
        "H, W = padded_img.shape\n",
        "kH, kW = kernel.shape\n",
        "outH = (H - kH)//stride + 1\n",
        "outW = (W - kW)//stride + 1\n",
        "out = np.zeros((outH, outW), dtype=np.float32)\n",
        "\n",
        "# ==========================\n",
        "# 2. Setup figura animata\n",
        "# ==========================\n",
        "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax1.set_title(\"Input + Kernel (con valori)\")\n",
        "ax2.set_title(\"Feature Map (valori aggiornati)\")\n",
        "\n",
        "im1 = ax1.imshow(padded_img, cmap='gray', vmin=0, vmax=1)\n",
        "im2 = ax2.imshow(out, cmap='gray', vmin=-4, vmax=4)\n",
        "\n",
        "# Rettangolo che rappresenta il kernel\n",
        "rect = plt.Rectangle((0, 0), kW-1, kH-1, edgecolor='red', facecolor='none', lw=2)\n",
        "ax1.add_patch(rect)\n",
        "\n",
        "for ax in (ax1, ax2):\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Testo dinamico\n",
        "text_kernel = fig.text(0.05, 0.02, \"\", fontsize=12, color='darkred')\n",
        "text_value = fig.text(0.55, 0.02, \"\", fontsize=12, color='darkblue')\n",
        "\n",
        "# Numeri della feature map\n",
        "texts_out = [[ax2.text(j, i, \"\", ha=\"center\", va=\"center\", color=\"white\", fontsize=10)\n",
        "              for j in range(outW)] for i in range(outH)]\n",
        "\n",
        "# Numeri del kernel che verranno visualizzati dentro il quadrato rosso\n",
        "texts_kernel = [[ax1.text(0, 0, \"\", ha=\"center\", va=\"center\", color=\"yellow\", fontsize=10)\n",
        "                 for _ in range(kW)] for _ in range(kH)]\n",
        "\n",
        "# ==========================\n",
        "# 3. Funzione di aggiornamento\n",
        "# ==========================\n",
        "def update(frame):\n",
        "    i = frame // outW\n",
        "    j = frame % outW\n",
        "\n",
        "    # estrai patch corrente\n",
        "    patch = padded_img[i*stride : i*stride + kH,\n",
        "                       j*stride : j*stride + kW]\n",
        "\n",
        "    value = np.sum(patch * kernel)\n",
        "    out[i, j] = value\n",
        "\n",
        "    # aggiorna rettangolo\n",
        "    rect.set_xy((j, i))\n",
        "    im2.set_data(out)\n",
        "\n",
        "    # aggiorna testi dinamici\n",
        "    text_kernel.set_text(f\"üß≠ Calcolando output[{i},{j}]\")\n",
        "    text_value.set_text(f\"üìà Œ£(patch √ó kernel) = {value:.2f}\")\n",
        "\n",
        "    # aggiorna numeri sulla feature map\n",
        "    for y in range(outH):\n",
        "        for x in range(outW):\n",
        "            texts_out[y][x].set_text(f\"{out[y,x]:.1f}\" if out[y,x] != 0 else \"\")\n",
        "\n",
        "    # aggiorna numeri del kernel (centrati nella posizione corrente)\n",
        "    for ki in range(kH):\n",
        "        for kj in range(kW):\n",
        "            x_pos = j + kj\n",
        "            y_pos = i + ki\n",
        "            texts_kernel[ki][kj].set_position((x_pos, y_pos))\n",
        "            texts_kernel[ki][kj].set_text(f\"{int(kernel[ki, kj])}\")\n",
        "\n",
        "    return [im1, im2, rect, text_kernel, text_value] + sum(texts_out, []) + sum(texts_kernel, [])\n",
        "\n",
        "# ==========================\n",
        "# 4. Animazione\n",
        "# ==========================\n",
        "ani = animation.FuncAnimation(\n",
        "    fig, update, frames=outH*outW, interval=1000, blit=False, repeat=True\n",
        ")\n",
        "\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "wf4HZiz-EBrZ"
      },
      "id": "wf4HZiz-EBrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e69a7ff6",
      "metadata": {
        "id": "e69a7ff6"
      },
      "source": [
        "# üß≠ Recap: cosa abbiamo fatto finora\n",
        "\n",
        "Nel blocco precedente abbiamo:\n",
        "\n",
        "1. **Creato un‚Äôimmagine artificiale 6√ó6**, quasi tutta nera, con un piccolo quadrato bianco al centro.\n",
        "‚Üí Serve per osservare visivamente come i filtri rispondono ai bordi.\n",
        "\n",
        "2. **Applicato una convoluzione 2D con un kernel(3√ó3)** che misura le variazioni di intensit√† orizzontali.\n",
        "\n",
        "Il nostro kernel:\n",
        "\n",
        "* colonne di sinistra positive (+1, +2, +1)\n",
        "\n",
        "* colonne di destra negative (‚àí1, ‚àí2, ‚àí1)\n",
        "‚Üí rileva bordi verticali, cio√® transizioni nero ‚Üî bianco.\n",
        "\n",
        "Visto l‚Äôeffetto di padding e stride:\n",
        "\n",
        "* Padding=0 ‚Üí l‚Äôimmagine ‚Äúsi restringe‚Äù\n",
        "\n",
        "* Padding=1 ‚Üí mantiene la stessa dimensione\n",
        "\n",
        "* Stride=2 ‚Üí dimezza l‚Äôoutput (downsampling)\n",
        "\n",
        "Visualizzato l‚Äôoutput della convoluzione:\n",
        "\n",
        "* Valori positivi (bianco) ‚Üí bordo da scuro ‚Üí chiaro\n",
        "\n",
        "* Valori negativi (nero) ‚Üí bordo da chiaro ‚Üí scuro\n",
        "\n",
        "* Valori vicini a 0 ‚Üí regioni uniformi senza bordi\n",
        "\n",
        "üëâ In pratica, l‚Äôoutput rappresenta dove il filtro ‚Äúvede‚Äù un bordo verticale.\n",
        "Ogni valore della feature map √® la risposta del filtro in quella posizione.\n",
        "\n",
        "## üß© Passaggio successivo ‚Äî Pooling\n",
        "\n",
        "Ora applichiamo un pooling sull‚Äôoutput della convoluzione per:\n",
        "\n",
        "1. ridurre la dimensione della feature map,\n",
        "\n",
        "2. preservare le caratteristiche principali,\n",
        "\n",
        "3. rendere il modello pi√π robusto a piccoli spostamenti.\n",
        "\n",
        "Esistono due tipi principali di pooling:\n",
        "\n",
        "* **Max Pooling:** prende il valore massimo di ogni blocco (conserva le attivazioni pi√π forti)\n",
        "\n",
        "* **Average Pooling:** prende la media di ogni blocco (effetto di ‚Äúlisciatura‚Äù)\n",
        "\n",
        "Visivamente ‚¨áÔ∏è\n",
        "\n",
        "![Alt text](https://miro.medium.com/v2/resize:fit:640/format:webp/1*sIf_zGTXgvTEDkpeP8jvxg.jpeg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üìò DIMOSTRAZIONE: Operatori di Manipolazione Tensors in PyTorch\n",
        "# ============================================================\n",
        "import torch\n",
        "\n",
        "# Creiamo un tensore di esempio\n",
        "x = torch.tensor([[[1], [2], [3]]])  # shape: (1, 3, 1)\n",
        "print(\"Tensor iniziale:\")\n",
        "print(x)\n",
        "print(\"Shape iniziale:\", x.shape)\n",
        "\n",
        "\n",
        "x_squeezed = torch.squeeze(x)\n",
        "print(\"\\nüîπ Dopo squeeze:\")\n",
        "print(x_squeezed)\n",
        "print(\"Shape:\", x_squeezed.shape)\n",
        "\n",
        "\n",
        "x_squeezed_dim0 = torch.squeeze(x, dim=0)\n",
        "print(\"\\nüîπ squeeze(dim=0):\", x_squeezed_dim0.shape)\n",
        "\n",
        "\n",
        "x_unsqueezed = torch.unsqueeze(x_squeezed, dim=0)\n",
        "print(\"\\nüîπ Dopo unsqueeze(dim=0):\")\n",
        "print(x_unsqueezed)\n",
        "print(\"Shape:\", x_unsqueezed.shape)\n",
        "\n",
        "\n",
        "x2 = torch.arange(1, 9)  # [1,2,...,8]\n",
        "print(\"\\nTensor base:\", x2)\n",
        "print(\"Shape:\", x2.shape)\n",
        "\n",
        "x2_view = x2.view(2, 4)\n",
        "x2_reshape = x2.reshape(4, 2)\n",
        "print(\"\\nüîπ view(2,4): shape\", x2_view.shape)\n",
        "print(\"üîπ reshape(4,2): shape\", x2_reshape.shape)\n",
        "\n",
        "\n",
        "x3 = torch.randn(2, 3, 4)\n",
        "print(\"\\nShape originale:\", x3.shape)\n",
        "\n",
        "x3_T = x3.transpose(0, 1)\n",
        "print(\"üîπ transpose(0,1):\", x3_T.shape)\n",
        "\n",
        "x3_P = x3.permute(2, 0, 1)\n",
        "print(\"üîπ permute(2,0,1):\", x3_P.shape)\n",
        "\n",
        "\n",
        "x4 = torch.tensor([[1], [2], [3]])  # shape: (3,1)\n",
        "print(\"\\nTensor base:\\n\", x4)\n",
        "\n",
        "x_expand = x4.expand(3, 4)\n",
        "x_repeat = x4.repeat(1, 4)\n",
        "\n",
        "print(\"\\nüîπ expand(3,4):\\n\", x_expand)\n",
        "print(\"Shape:\", x_expand.shape)\n",
        "\n",
        "print(\"\\nüîπ repeat(1,4):\\n\", x_repeat)\n",
        "print(\"Shape:\", x_repeat.shape)\n",
        "\n",
        "\n",
        "print(\"\\nüìä Riepilogo finale:\")\n",
        "print(\"x shape:\", x.shape)\n",
        "print(\"squeeze ‚Üí\", x_squeezed.shape)\n",
        "print(\"unsqueeze ‚Üí\", x_unsqueezed.shape)\n",
        "print(\"view ‚Üí\", x2_view.shape)\n",
        "print(\"transpose ‚Üí\", x3_T.shape)\n",
        "print(\"permute ‚Üí\", x3_P.shape)\n",
        "print(\"expand ‚Üí\", x_expand.shape)\n",
        "print(\"repeat ‚Üí\", x_repeat.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "6U3To7kEmoAL"
      },
      "id": "6U3To7kEmoAL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd76ec0",
      "metadata": {
        "id": "edd76ec0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Convertiamo l'output della convoluzione (out_pad_1, un array NumPy 6x6) in un TENSORE PyTorch.\n",
        "# Le reti convoluzionali in PyTorch si aspettano input con forma:\n",
        "# (N, C, H, W)\n",
        "# dove:\n",
        "#   N = numero di esempi (batch size)\n",
        "#   C = numero di canali (es. 1 per immagini in scala di grigi, 3 per RGB)\n",
        "#   H = altezza (height)\n",
        "#   W = larghezza (width)\n",
        "\n",
        "# out_pad_1 ha shape (6, 6) ‚Üí solo H e W.\n",
        "# Per renderlo compatibile, aggiungiamo:\n",
        "# - una dimensione per il batch (N=1)\n",
        "# - una per il canale (C=1)\n",
        "# Risultato: (1, 1, 6, 6)\n",
        "feature_map = torch.tensor(out_pad_1, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "print(\"Shape feature map:\", feature_map.shape)  # (1, 1, 6, 6)\n",
        "\n",
        "# Definiamo i due tipi di pooling:\n",
        "# MaxPool2d: seleziona il valore massimo in ogni finestra (kernel 2x2)\n",
        "# AvgPool2d: calcola la media dei valori nella stessa finestra\n",
        "# Lo stride=2 significa che ogni finestra \"salta\" di 2 pixel ‚Üí dimezza H e W (6x6 ‚Üí 3x3)\n",
        "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "# Applichiamo il pooling alla feature map\n",
        "max_pooled = maxpool(feature_map)  # mantiene la forma (1,1,3,3)\n",
        "avg_pooled = avgpool(feature_map)\n",
        "\n",
        "# Stampiamo le forme per verificare la riduzione spaziale\n",
        "print(\"Max Pooling output shape:\", max_pooled.shape)\n",
        "print(\"Average Pooling output shape:\", avg_pooled.shape)\n",
        "\n",
        "# Visualizziamo i risultati a confronto:\n",
        "# - Feature map originale (output della convoluzione)\n",
        "# - Max pooling (3x3): mantiene solo i valori massimi locali\n",
        "# - Average pooling (3x3): calcola la media locale, smussando i dettagli\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "\n",
        "axes[0].imshow(out_pad_1, cmap='gray')\n",
        "axes[0].set_title(\"Feature map originale (6√ó6)\")\n",
        "\n",
        "axes[1].imshow(max_pooled.squeeze(), cmap='gray')\n",
        "axes[1].set_title(\"Max pooling (3√ó3)\")\n",
        "\n",
        "axes[2].imshow(avg_pooled.squeeze(), cmap='gray')\n",
        "axes[2].set_title(\"Average pooling (3√ó3)\")\n",
        "\n",
        "# Rimuoviamo gli assi per una visualizzazione pi√π pulita\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Cosa osserviamo\n",
        "\n",
        "* Dopo il **Max Pooling**, l‚Äôimmagine √® pi√π piccola (6√ó6 ‚Üí 3√ó3) ma i bordi principali sono ancora ben evidenti.\n",
        "\n",
        "* Dopo **l‚ÄôAverage Pooling**, le variazioni si ‚Äúammorbidiscono‚Äù: le intensit√† estreme vengono mediate.\n",
        "\n",
        "‚úÖ Conclusione\n",
        "\n",
        "Il pooling serve per:\n",
        "\n",
        "* ridurre la quantit√† di dati da elaborare nei layer successivi,\n",
        "\n",
        "* mantenere solo le informazioni pi√π significative,\n",
        "\n",
        "* garantire invarianza a traslazioni locali (es. un bordo spostato di 1 pixel non cambia molto l‚Äôattivazione del blocco).\n",
        "\n",
        "In una CNN reale, dopo ogni convoluzione si alternano tipicamente:\n",
        "\n",
        "**Conv ‚Üí ReLU ‚Üí Pooling**\n",
        "\n",
        "per costruire progressivamente rappresentazioni pi√π compatte ma pi√π ricche di significato."
      ],
      "metadata": {
        "id": "KYmpFwgxjA0O"
      },
      "id": "KYmpFwgxjA0O"
    },
    {
      "cell_type": "markdown",
      "id": "93c9a290",
      "metadata": {
        "id": "93c9a290"
      },
      "source": [
        "\n",
        "## La tua prima CNN (MNIST)\n",
        "\n",
        "Costruiremo una piccola CNN per **MNIST** (cifre 28√ó28 in scala di grigi).  \n",
        "**Pipeline:**\n",
        "1. **Transforms**: conversione a tensore e **normalizzazione** (media=0.1307, std=0.3081).  \n",
        "2. **Dataset & DataLoader** per train/val/test.  \n",
        "3. **Modello**: due blocchi conv ‚Üí flatten ‚Üí fully connected.  \n",
        "4. **Training loop** con `CrossEntropyLoss` + `Adam`.  \n",
        "5. **Accuratezza** e barra `tqdm` per il progresso.  \n",
        "6. **Grafici** di loss e accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beaf6715",
      "metadata": {
        "id": "beaf6715"
      },
      "outputs": [],
      "source": [
        "# 3.1 Trasformazioni (normalizzazione consigliata per MNIST)\n",
        "# Compose concatena pi√π trasformazioni da applicare in sequenza a ogni immagine.\n",
        "# ToTensor: converte un'immagine PIL/array [0..255] in un tensore float [0..1] con shape (C,H,W).\n",
        "# Normalize: per ogni canale applica (x - mean) / std. Per MNIST (grayscale) c'√® un solo canale.\n",
        "#            mean=0.1307 e std=0.3081 sono statistiche standard del dataset MNIST.\n",
        "transform_mnist = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # media e std note per MNIST\n",
        "])\n",
        "\n",
        "# 3.2 Dataset (con fallback)\n",
        "# Directory locale dove scaricare o cercare i file del dataset.\n",
        "data_root = \"./data\"\n",
        "\n",
        "# Proviamo a caricare/scaricare MNIST:\n",
        "# - train=True: split di addestramento (60k immagini)\n",
        "# - train=False: split di test (10k immagini)\n",
        "# - transform=transform_mnist: applica le trasformazioni definite sopra\n",
        "# - download=True: scarica se non √® gi√† presente nella cartella data_root\n",
        "try:\n",
        "    train_full = datasets.MNIST(root=data_root, train=True,  transform=transform_mnist, download=True)\n",
        "    test_ds    = datasets.MNIST(root=data_root, train=False, transform=transform_mnist, download=True)\n",
        "except Exception as e:\n",
        "    # In caso di assenza di rete o problemi di I/O, si entra qui.\n",
        "    # ATTENZIONE: con questo solo print, train_full e test_ds rimangono NON definiti.\n",
        "    # Pi√π sotto trovi una variante robusta che crea un vero fallback.\n",
        "    print(\"‚ö†Ô∏è Impossibile scaricare MNIST. Errore:\", str(e))\n",
        "\n",
        "# 3.3 Split train/val\n",
        "# Percentuale del training set da usare come validazione (10%).\n",
        "val_ratio = 0.1\n",
        "# Numero di esempi destinati alla validazione.\n",
        "val_size = int(len(train_full) * val_ratio)\n",
        "# Numero di esempi destinati al training vero e proprio.\n",
        "train_size = len(train_full) - val_size\n",
        "# random_split crea due sottoinsiemi casuali del dataset originale.\n",
        "# (La casualit√† √® controllata dal seed impostato in precedenza per riproducibilit√†.)\n",
        "train_ds, val_ds = random_split(train_full, [train_size, val_size])\n",
        "\n",
        "# 3.4 DataLoader\n",
        "# Dimensione dei batch (128 √® un buon compromesso per MNIST; puoi aumentarla con GPU pi√π capienti).\n",
        "batch_size = 128\n",
        "# DataLoader per il training: shuffle=True rimescola i campioni a ogni epoca (buona pratica).\n",
        "# num_workers=2 usa due processi per il prefetch dei dati (aumenta throughput, valuta in base alla macchina).\n",
        "# pin_memory=True su CUDA fissa le pagine in RAM per copie host‚Üídevice pi√π rapide.\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True if device.type=='cuda' else False)\n",
        "# DataLoader per la validazione: niente shuffle (ordine stabile).\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True if device.type=='cuda' else False)\n",
        "# DataLoader per il test: niente shuffle (ordine stabile).\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True if device.type=='cuda' else False)\n",
        "\n",
        "# In un notebook, l'ultima espressione non assegnata viene mostrata come output della cella.\n",
        "# Qui riportiamo le cardinalit√† dei tre split e una stringa \"MNIST\" come etichetta di comodo.\n",
        "len(train_ds), len(val_ds), len(test_ds), \"MNIST\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1bfc66",
      "metadata": {
        "id": "3c1bfc66"
      },
      "outputs": [],
      "source": [
        "# 3.5 Visualizziamo alcune immagini con le etichette\n",
        "def show_batch(dl, n=5):\n",
        "    # Stampa la lunghezza del DataLoader, ovvero il numero totale di batch.\n",
        "    # Esempio: se il dataset ha 54.000 immagini e batch_size=128 ‚Üí ci saranno circa 422 batch.\n",
        "    print(len(dl))\n",
        "\n",
        "    # next(iter(dl)) estrae il PRIMO batch dal DataLoader.\n",
        "    # xb = immagini (batch di tensori)\n",
        "    # yb = etichette (batch di target numerici)\n",
        "    xb, yb = next(iter(dl))\n",
        "\n",
        "    # Stampa le shape dei tensori per capire come sono strutturati.\n",
        "    print(xb.shape)  # (batch_size, canali, altezza, larghezza)\n",
        "    print(yb.shape)  # (batch_size,)\n",
        "\n",
        "    # Seleziona solo i primi n campioni del batch (es. 5 immagini)\n",
        "    xb, yb = xb[:n], yb[:n]\n",
        "\n",
        "    # Sposta i dati su CPU e li converte in array NumPy per poterli plottare con Matplotlib.\n",
        "    xb = xb.cpu().numpy()\n",
        "\n",
        "    # Crea una nuova figura.\n",
        "    plt.figure()\n",
        "\n",
        "    # cols = numero di colonne nel layout dei subplot (una riga con n immagini)\n",
        "    cols = n\n",
        "    for i in range(n):\n",
        "        # Crea una subplot 1√ón, posizione i+1\n",
        "        plt.subplot(1, cols, i+1)\n",
        "\n",
        "        # xb[i][0]: seleziona la i-esima immagine e il primo canale (0, perch√© √® in bianco e nero)\n",
        "        plt.imshow(xb[i][0], cmap='gray')\n",
        "\n",
        "        # Mostra il numero della classe come titolo\n",
        "        plt.title(f\"Label: {int(yb[i])}\")\n",
        "\n",
        "        # Nasconde gli assi per una visualizzazione pi√π pulita\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Mostra tutte le immagini insieme\n",
        "    plt.show()\n",
        "\n",
        "# Esegui la funzione per visualizzare 5 immagini dal training set\n",
        "show_batch(train_loader, n=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f826f6b5",
      "metadata": {
        "id": "f826f6b5"
      },
      "outputs": [],
      "source": [
        "# 3.6 Definiamo la CNN\n",
        "class SimpleCNN(nn.Module):  # Definiamo una rete neurale come sottoclasse di nn.Module (base class PyTorch per i modelli)\n",
        "    def __init__(self, p_dropout=0.0, use_batchnorm=False):\n",
        "        super().__init__()  # Inizializza la parte nn.Module (registrazione dei sottolayer, ecc.)\n",
        "\n",
        "        # c1 e c2 = numero di canali (feature maps) prodotti dai due blocchi convoluzionali.\n",
        "        # Valori piccoli (5 e 10) per chiarezza didattica: pi√π canali = pi√π capacit√†, pi√π costi.\n",
        "        c1, c2 = 5, 10\n",
        "\n",
        "        # Costruiamo progressivamente la \"pila\" di layer convoluzionali/attivazioni/pooling.\n",
        "        layers = []\n",
        "\n",
        "        # --- BLOCCO CONV 1 ---\n",
        "        # nn.Conv2d(in_channels=1, out_channels=c1, kernel_size=3, padding=1)\n",
        "        # - in_channels=1: immagini MNIST sono 1-canale (grayscale).\n",
        "        # - out_channels=c1: n¬∞ di filtri appresi (uscita: c1 mappe di attivazione).\n",
        "        # - kernel_size=3: filtri 3x3 (standard).\n",
        "        # - padding=1: \"same-ish\": mantiene la dimensione spaziale (28 -> 28) con kernel=3.\n",
        "        layers += [nn.Conv2d(1, c1, kernel_size=3, padding=1),\n",
        "                   nn.ReLU(inplace=True)]  # ReLU rende non lineare. inplace=True: risparmia memoria (sovrascrive input).\n",
        "\n",
        "        # Facoltativo: BatchNorm2d stabilizza/accelera il training normalizzando le attivazioni per canale.\n",
        "        # (Si applica dopo conv e prima/ dopo ReLU; qui dopo ReLU per semplicit√† didattica)\n",
        "        if use_batchnorm:\n",
        "            layers += [nn.BatchNorm2d(c1)]\n",
        "\n",
        "        # MaxPool2d(2,2): dimezza le dimensioni spaziali (H, W) prendendo il massimo in finestre 2x2.\n",
        "        # 28x28 -> 14x14\n",
        "        layers += [nn.MaxPool2d(2, 2)]  # 28->14\n",
        "\n",
        "        # --- BLOCCO CONV 2 ---\n",
        "        # Seconda convoluzione:\n",
        "        # - in_channels=c1 (quanti canali escono dal blocco 1)\n",
        "        # - out_channels=c2 (nuove mappe di attivazione)\n",
        "        # - padding=1 mantiene 14x14 in ingresso -> 14x14 in uscita prima del pooling\n",
        "        layers += [nn.Conv2d(c1, c2, kernel_size=3, padding=1),\n",
        "                   nn.ReLU(inplace=True)]\n",
        "\n",
        "        if use_batchnorm:\n",
        "            layers += [nn.BatchNorm2d(c2)]\n",
        "\n",
        "        # Secondo MaxPool: 14x14 -> 7x7\n",
        "        layers += [nn.MaxPool2d(2, 2)]  # 14->7\n",
        "\n",
        "        # Raggruppiamo i layer feature-extractor in un Sequential per rendere il forward compatto/leggibile.\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Dropout sul vettore flatten prima del classificatore:\n",
        "        # - p_dropout=0.0: disattivato; >0.0 attivato (es. 0.3)\n",
        "        # - Dropout aiuta a ridurre overfitting azzerando casualmente alcune attivazioni a train-time.\n",
        "        # - nn.Identity() √® un \"layer no-op\" (fa pass-through) quando non vogliamo dropout.\n",
        "        self.dropout = nn.Dropout(p_dropout) if p_dropout > 0 else nn.Identity()\n",
        "\n",
        "        # Layer fully-connected (classificatore):\n",
        "        # - Input atteso: vettore di dimensione 7*7*c2 (dopo i due MaxPool).\n",
        "        #   * Partiamo da 28x28 -> Conv (28x28) -> Pool (14x14) -> Conv (14x14) -> Pool (7x7)\n",
        "        #   * Canali finali = c2\n",
        "        # - Output: 10 classi (cifre 0..9) per MNIST.\n",
        "        self.classifier = nn.Linear(7 * 7 * c2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x forma attesa: (N, 1, 28, 28)  N=batch size, 1=grayscale\n",
        "        x = self.features(x)          # Applica i blocchi Conv/ReLU/(BN)/Pool -> output: (N, c2, 7, 7)\n",
        "        x = torch.flatten(x, 1)       # Flatten a partire dalla dim=1 (mantiene N): (N, 7*7*c2)\n",
        "        x = self.dropout(x)           # Eventuale dropout (se p_dropout>0), altrimenti pass-through\n",
        "        x = self.classifier(x)        # Linear -> logits di dimensione 10 (uno per classe)\n",
        "        return x                      # Ritorniamo logits (NON softmax). CrossEntropyLoss applica softmax internamente.\n",
        "\n",
        "def count_params(model):\n",
        "    # Conta il numero TOTALE di parametri allenabili (requires_grad=True).\n",
        "    # p.numel() = n¬∞ elementi del tensore (es. un peso per ogni connessione/filtro).\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Istanzia il modello:\n",
        "# - p_dropout=0.0: nessun dropout (metti 0.3 per iniziare a regolarizzare)\n",
        "# - use_batchnorm=False: BN disattivata (metti True per attivarla)\n",
        "# - .to(device): sposta i pesi su GPU se disponibile, altrimenti CPU\n",
        "model = SimpleCNN(p_dropout=0.0, use_batchnorm=False).to(device)\n",
        "\n",
        "print(model)  # Stampa l‚Äôarchitettura leggibile (layer per layer con shape implicite)\n",
        "\n",
        "# Conta e stampa quanti parametri verranno effettivamente aggiornati durante il training\n",
        "print(\"Parametri allenabili:\", count_params(model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833dd572",
      "metadata": {
        "id": "833dd572"
      },
      "outputs": [],
      "source": [
        "# Calcola l'accuratezza a partire dai logits (uscita del modello) e dalle label vere y\n",
        "def accuracy(logits, y):\n",
        "    # logits: tensore di forma (N, C) ‚Äî N=batch size, C=numero classi (es. 10)\n",
        "    # argmax(dim=1): prende l'indice della classe con logit pi√π alto per ciascun esempio ‚Üí shape (N,)\n",
        "    preds = logits.argmax(dim=1)\n",
        "    # (preds == y): tensore booleano (N,) True/False per ogni esempio\n",
        "    # .float(): converte True‚Üí1.0, False‚Üí0.0\n",
        "    # .mean(): media sui N esempi ‚Üí accuratezza in [0,1]\n",
        "    # .item(): estrae lo scalare Python dal tensore (utile per log / accumulo)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "\n",
        "# Esegue UNA epoca di training (forward + backward + update pesi) e restituisce loss/accuracy medie sull'epoca\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()  # modalit√† training: attiva dropout, BatchNorm usa statistiche di batch, ecc.\n",
        "    running_loss, running_acc = 0.0, 0.0  # accumulatori per somma dei contributi (poi faremo la media pesata)\n",
        "    # tqdm: barra di avanzamento sul DataLoader; desc √® il testo mostrato; leave=False non lascia la barra dopo il loop\n",
        "    for xb, yb in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        # Sposta batch (immagini) e label sul device corretto (CPU/GPU)\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()   # azzera i gradienti accumulati dal passo precedente (PyTorch li accumula di default)\n",
        "\n",
        "        logits = model(xb)      # forward pass: otteniamo i logits (shape (N, C))\n",
        "        loss = criterion(logits, yb)  # calcolo della loss (es. CrossEntropyLoss sui logits e le label)\n",
        "\n",
        "        loss.backward()         # backpropagation: calcola i gradienti dL/dŒ∏\n",
        "        optimizer.step()        # aggiornamento dei pesi secondo la regola dell‚Äôottimizzatore (Adam/SGD, ecc.)\n",
        "\n",
        "        # Accumulo della SOMMA della loss pesata per il numero di esempi nel batch\n",
        "        # .item() estrae lo scalare Python; xb.size(0) = N del batch (ultimo batch pu√≤ essere pi√π piccolo)\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        # Accumulo della SOMMA delle accuratezze per esempio (accuracy(batch)*N)\n",
        "        # In questo modo la media finale √® correttamente pesata per la dimensione dei batch\n",
        "        running_acc  += accuracy(logits, yb) * xb.size(0)\n",
        "\n",
        "    n = len(loader.dataset)      # numero totale di esempi visti in questa epoca (tutti i batch)\n",
        "    # Ritorniamo le MEDIE su tutti gli esempi (loss e accuracy micro-averaged)\n",
        "    return running_loss / n, running_acc / n\n",
        "\n",
        "\n",
        "# Valutazione (validazione/test): NESSUN gradiente, NESSUN update pesi\n",
        "@torch.no_grad()  # disattiva il tracciamento dei gradienti all'interno della funzione (meno memoria, pi√π veloce)\n",
        "def evaluate(model, loader, criterion, phase=\"Val\"):\n",
        "    model.eval()  # modalit√† eval: disattiva dropout; BatchNorm usa le running stats (non quelle del batch)\n",
        "    running_loss, running_acc = 0.0, 0.0\n",
        "    # Loop sui batch del loader (val o test); tqdm mostra \"Val\" o \"Test\" a seconda del parametro\n",
        "    for xb, yb in tqdm(loader, desc=phase, leave=False):\n",
        "        xb, yb = xb.to(device), yb.to(device)   # sposta dati su device\n",
        "        logits = model(xb)                      # forward pass (solo inferenza)\n",
        "        loss = criterion(logits, yb)            # calcolo loss di validazione/test\n",
        "\n",
        "        # Accumuli come nel training, ma senza backward/step\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "        running_acc  += accuracy(logits, yb) * xb.size(0)\n",
        "\n",
        "    n = len(loader.dataset)  # numero totale di esempi del set (val/test)\n",
        "    # Medie su tutto il dataset (corrette anche se l'ultimo batch √® pi√π piccolo)\n",
        "    return running_loss / n, running_acc / n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7a5786",
      "metadata": {
        "id": "9a7a5786"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3.8 Addestriamo!\n",
        "epochs = 3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0)  # weight_decay √® la L2\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)  # semplice scheduler LR\n",
        "\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f\"\\nEpoca {epoch}/{epochs}\")\n",
        "    tl, ta = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    vl, va = evaluate(model, val_loader, criterion, phase=\"Val\")\n",
        "    scheduler.step()\n",
        "\n",
        "    history[\"train_loss\"].append(tl); history[\"train_acc\"].append(ta)\n",
        "    history[\"val_loss\"].append(vl);   history[\"val_acc\"].append(va)\n",
        "\n",
        "    print(f\"loss_train={tl:.4f}  acc_train={ta:.4f} | loss_val={vl:.4f}  acc_val={va:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c3a071",
      "metadata": {
        "id": "60c3a071"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3.9 Grafici delle curve di apprendimento (matplotlib; un grafico per figura; nessuno stile colore specifico)\n",
        "plt.figure()\n",
        "plt.plot(history[\"train_loss\"], label=\"loss_train\")\n",
        "plt.plot(history[\"val_loss\"],   label=\"loss_val\")\n",
        "plt.legend(); plt.xlabel(\"epoca\"); plt.ylabel(\"loss\"); plt.title(\"Andamento della loss\"); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history[\"train_acc\"], label=\"acc_train\")\n",
        "plt.plot(history[\"val_acc\"],   label=\"acc_val\")\n",
        "plt.legend(); plt.xlabel(\"epoca\"); plt.ylabel(\"accuratezza\"); plt.title(\"Andamento dell'accuratezza\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae98826f",
      "metadata": {
        "id": "ae98826f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3.10 Valutazione sul test set + alcune predizioni\n",
        "@torch.no_grad()\n",
        "def get_predictions(model, loader, max_batches=1):\n",
        "    model.eval()\n",
        "    images, labels, preds = [], [], []\n",
        "    batches_done = 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        pb = logits.argmax(dim=1).cpu()\n",
        "        images.append(xb.cpu())\n",
        "        labels.append(yb)\n",
        "        preds.append(pb)\n",
        "        batches_done += 1\n",
        "        if batches_done >= max_batches:\n",
        "            break\n",
        "    return torch.cat(images, dim=0), torch.cat(labels, dim=0), torch.cat(preds, dim=0)\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, nn.CrossEntropyLoss(), phase=\"Test\")\n",
        "print(f\"Test: loss={test_loss:.4f}, acc={test_acc:.4f}\")\n",
        "\n",
        "imgs, labs, prds = get_predictions(model, test_loader, max_batches=1)\n",
        "n = min(8, imgs.size(0))\n",
        "plt.figure()\n",
        "for i in range(n):\n",
        "    plt.subplot(1, n, i+1)\n",
        "    plt.imshow(imgs[i,0], cmap='gray')\n",
        "    plt.title(f\"T:{int(labs[i])}/P:{int(prds[i])}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563b591b",
      "metadata": {
        "id": "563b591b"
      },
      "source": [
        "\n",
        "## 4. Regolarizzazione e miglioramenti\n",
        "\n",
        "**Perch√© servono?** Per ridurre **overfitting** e rendere l'addestramento pi√π stabile.\n",
        "\n",
        "- **Dropout:** azzera casualmente alcune attivazioni durante il training (es. `p=0.3`).  \n",
        "- **Weight decay (L2):** penalizza pesi grandi tramite l'ottimizzatore (es. `weight_decay=1e-4`).  \n",
        "- **Batch Normalization:** normalizza le attivazioni per velocizzare/stabilizzare il training.  \n",
        "- **Scheduler LR:** regola il learning rate durante l'addestramento (es. `StepLR`, `OneCycleLR`).\n",
        "\n",
        "Sotto attiviamo **dropout + batch norm** e facciamo un breve training dimostrativo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaea0e26",
      "metadata": {
        "id": "eaea0e26"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_bn = SimpleCNN(p_dropout=0.3, use_batchnorm=True).to(device)\n",
        "optimizer = optim.Adam(model_bn.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f\"\\n[BN+Dropout] Epoca {epoch}/{epochs}\")\n",
        "    tl, ta = train_one_epoch(model_bn, train_loader, optimizer, criterion)\n",
        "    vl, va = evaluate(model_bn, val_loader, criterion, phase=\"Val\")\n",
        "    print(f\"loss_train={tl:.4f}  acc_train={ta:.4f} | loss_val={vl:.4f}  acc_val={va:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d86bdb",
      "metadata": {
        "id": "26d86bdb"
      },
      "source": [
        "\n",
        "## 4bis. Confusion Matrix ed Early Stopping\n",
        "\n",
        "In questa sezione impariamo due strumenti fondamentali per migliorare l‚Äôanalisi e la robustezza del modello:\n",
        "\n",
        "1. **Confusion Matrix (Matrice di confusione)**  \n",
        "   Serve per capire **quali classi il modello confonde** tra loro.  \n",
        "   Ogni cella mostra quante volte una classe reale √® stata predetta come un‚Äôaltra classe.\n",
        "\n",
        "   - La diagonale rappresenta le **predizioni corrette**.  \n",
        "   - Le celle fuori diagonale mostrano gli **errori di classificazione**.\n",
        "\n",
        "2. **Early Stopping (Interruzione anticipata)**  \n",
        "   Serve a fermare l‚Äôaddestramento **prima** che il modello inizi a sovradattarsi (*overfitting*).  \n",
        "   Se la **loss di validazione** non migliora dopo un certo numero di epoche (pazienza), si interrompe il training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ed0c8d",
      "metadata": {
        "id": "e4ed0c8d"
      },
      "outputs": [],
      "source": [
        "# Importiamo due utility da scikit-learn:\n",
        "# - confusion_matrix: calcola la matrice di confusione a partire da etichette vere e predette\n",
        "# - ConfusionMatrixDisplay: oggetto comodo per visualizzare la matrice con Matplotlib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Disattiviamo il tracciamento del gradiente dentro questa funzione (niente backprop durante la valutazione):\n",
        "@torch.no_grad()\n",
        "def compute_confusion_matrix(model, loader):\n",
        "    model.eval()                       # Mettiamo il modello in modalit√† 'eval' (disattiva dropout, BN in mode eval, ecc.)\n",
        "    all_preds, all_labels = [], []     # Liste dove accumuliamo predizioni e label di TUTTI i batch\n",
        "\n",
        "    # Iteriamo su tutto il DataLoader (tipicamente test_loader)\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)   # Spostiamo batch e label sul device corretto (CPU/GPU)\n",
        "        preds = model(xb).argmax(1)             # Forward pass ‚Üí logits; argmax(1) prende la classe con logit pi√π alto per ogni immagine\n",
        "        all_preds.append(preds.cpu())           # Portiamo su CPU e accodiamo le predizioni del batch\n",
        "        all_labels.append(yb.cpu())             # Portiamo su CPU e accodiamo le label vere del batch\n",
        "\n",
        "    # Concateniamo tutte le predizioni/etichette in un unico tensore (dimensione totale = n. esempi del loader)\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Calcoliamo la matrice di confusione: righe = etichetta vera, colonne = etichetta predetta\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return cm\n",
        "\n",
        "# --- Calcolo e visualizzazione della matrice di confusione sul TEST set ---\n",
        "\n",
        "cm = compute_confusion_matrix(model, test_loader)       # Otteniamo la matrice (10x10 per MNIST)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,      # Prepariamo l'oggetto di visualizzazione\n",
        "                              display_labels=list(range(10)))  # Etichette degli assi: 0..9\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")              # Disegniamo la heatmap in blu; 'd' = interi nelle celle\n",
        "plt.title(\"Matrice di confusione - MNIST\")              # Titolo del grafico\n",
        "plt.show()                                              # Mostriamo la figura\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ef6e2f",
      "metadata": {
        "id": "b9ef6e2f"
      },
      "outputs": [],
      "source": [
        "# Early Stopping semplice: si interrompe se la loss di validazione non migliora per 'patience' epoche consecutive\n",
        "\n",
        "def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=20, patience=3, min_delta=0.05 ):#1e-3):\n",
        "    # Inizializziamo la \"migliore\" loss di validazione con infinito (qualsiasi valore reale sar√† pi√π piccolo)\n",
        "    best_val_loss = float('inf')\n",
        "    # Contatore di epoche consecutive senza miglioramento\n",
        "    counter = 0\n",
        "    # Dizionario per tenere traccia delle metriche nel tempo (per plotting/analisi)\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())  # Pesi iniziali (in caso non migliori mai)\n",
        "\n",
        "    # Loop sulle epoche\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Messaggio di stato per chiarezza in output\n",
        "        print(f\"\\nEpoca {epoch}/{epochs}\")\n",
        "\n",
        "        # Eseguiamo un'epoca di training (funzione definita prima: imposta model.train(), fa i batch, backprop, ecc.)\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "        # Eseguiamo la valutazione sul validation set (funzione definita prima: model.eval(), no grad, calcolo loss/acc)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, phase='Val')\n",
        "\n",
        "        # Se abbiamo uno scheduler di learning rate \"per epoca\", lo facciamo avanzare qui\n",
        "        if scheduler: scheduler.step()\n",
        "\n",
        "        # Salviamo le metriche dell'epoca nel nostro storico\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        # Log leggibile della qualit√† raggiunta in questa epoca\n",
        "        print(f\"loss_train={train_loss:.4f} | loss_val={val_loss:.4f}\")\n",
        "\n",
        "        # -------------------------\n",
        "        #      EARLY STOPPING\n",
        "        # -------------------------\n",
        "        # --- Early Stopping Logic ---\n",
        "        improvement = best_val_loss - val_loss\n",
        "        # Se la loss di validazione √® migliorata, azzeriamo il contatore e \"salviamo\" i pesi migliori\n",
        "        if improvement > min_delta:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            print(f\"‚úÖ Miglioramento rilevato: nuova best_val_loss = {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            # Nessun miglioramento sufficiente\n",
        "            counter += 1\n",
        "            print(f\"‚ö†Ô∏è Nessun miglioramento significativo ({counter}/{patience})\")\n",
        "            if counter >= patience:\n",
        "                print(f\"üõë Early Stopping attivato dopo {epoch} epoche! Miglior val_loss = {best_val_loss:.4f}\")\n",
        "                model.load_state_dict(best_model_wts)\n",
        "                break\n",
        "\n",
        "    # Ritorniamo il modello (con i migliori pesi) e lo storico delle metriche\n",
        "    return model, history\n",
        "\n",
        "# Esempio pratico (allenamento breve con early stopping)\n",
        "# Istanzio una SimpleCNN con un po' di regolarizzazione (Dropout) e BatchNorm attiva\n",
        "model_es = SimpleCNN(p_dropout=0.2, use_batchnorm=True).to(device)\n",
        "# Ottimizzatore Adam con LR moderato\n",
        "optimizer = optim.Adam(model_es.parameters(), lr=3e-4)\n",
        "# Loss per classificazione multi-classe: accetta logits e applica softmax internamente\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Alleno per max 10 epoche, ma fermo prima se per 2 epoche di fila la val_loss non migliora\n",
        "model_es, hist_es = train_with_early_stopping(model_es, train_loader, val_loader, criterion, optimizer, epochs=10, patience=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e155aaf",
      "metadata": {
        "id": "9e155aaf"
      },
      "source": [
        "\n",
        "### Visualizzazione delle curve di Early Stopping\n",
        "\n",
        "Nei grafici seguenti vediamo come si comportano **loss** e **accuratezza** nel tempo:  \n",
        "- le curve blu rappresentano l‚Äôandamento del **training**;  \n",
        "- le curve arancioni rappresentano l‚Äô**insieme di validazione**;  \n",
        "- il **punto rosso** indica il momento in cui l‚Äôearly stopping ha interrotto l‚Äôaddestramento.\n",
        "\n",
        "Osservando i grafici puoi capire quando il modello ha smesso di migliorare sulla validazione, pur continuando a migliorare (overfitting) sul training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a69fb58",
      "metadata": {
        "id": "5a69fb58"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Troviamo l'epoca di stop (ultima epoca effettiva eseguita)\n",
        "stopped_epoch = len(hist_es[\"val_loss\"]) - 1\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist_es[\"train_loss\"], label=\"loss_train\")\n",
        "plt.plot(hist_es[\"val_loss\"], label=\"loss_val\")\n",
        "plt.scatter(stopped_epoch, hist_es[\"val_loss\"][stopped_epoch], color=\"red\", label=\"stop\")\n",
        "plt.legend(); plt.xlabel(\"epoca\"); plt.ylabel(\"loss\")\n",
        "plt.title(\"Early Stopping - Andamento della loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(hist_es[\"train_acc\"], label=\"acc_train\")\n",
        "plt.plot(hist_es[\"val_acc\"], label=\"acc_val\")\n",
        "plt.scatter(stopped_epoch, hist_es[\"val_acc\"][stopped_epoch], color=\"red\", label=\"stop\")\n",
        "plt.legend(); plt.xlabel(\"epoca\"); plt.ylabel(\"accuratezza\")\n",
        "plt.title(\"Early Stopping - Andamento dell'accuratezza\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f34deaa8",
      "metadata": {
        "id": "f34deaa8"
      },
      "source": [
        "\n",
        "## 5. Salvataggio e caricamento del modello\n",
        "\n",
        "Usa `state_dict` per salvare solo i pesi:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec44e718",
      "metadata": {
        "id": "ec44e718"
      },
      "outputs": [],
      "source": [
        "\n",
        "save_path = \"simple_cnn_mnist.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Salvato in {save_path}\")\n",
        "\n",
        "# Per caricare:\n",
        "loaded = SimpleCNN()\n",
        "loaded.load_state_dict(torch.load(save_path, map_location=\"cpu\"))\n",
        "loaded.eval()\n",
        "print(\"Pesi caricati in un nuovo modello.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb62a42",
      "metadata": {
        "id": "fcb62a42"
      },
      "source": [
        "\n",
        "## 6. (Opzionale) Assaggio di transfer learning (CIFAR-10)\n",
        "\n",
        "**Idea:** partire da un modello pre-addestrato su un grande dataset (es. ImageNet) e adattarlo al tuo compito.\n",
        "\n",
        "Useremo **ResNet18** da `torchvision.models`. Se non √® possibile scaricare il dataset, ricorriamo a `FakeData` (3√ó32√ó32, ridimensionate a 224√ó224).  \n",
        "√à solo un assaggio: per risultati solidi servono pi√π epoche e un setup pi√π curato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f480eb8b",
      "metadata": {
        "id": "f480eb8b"
      },
      "outputs": [],
      "source": [
        "# Importiamo la ResNet18 e i pesi pre-addestrati disponibili in torchvision\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "# ----------------------------\n",
        "# 1Ô∏è‚É£ Definizione delle trasformazioni\n",
        "# ----------------------------\n",
        "transform_cifar = transforms.Compose([\n",
        "    # CIFAR-10 ha immagini 32x32, ma ResNet18 si aspetta input 224x224 (ImageNet)\n",
        "    transforms.Resize((224, 224)),\n",
        "\n",
        "    # Converte l'immagine PIL in tensore PyTorch con valori [0,1]\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Normalizzazione con le stesse statistiche (mean/std) di ImageNet\n",
        "    # fondamentale per far funzionare bene i pesi pre-addestrati\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# 2Ô∏è‚É£ Caricamento dataset CIFAR-10 (con fallback)\n",
        "# ----------------------------\n",
        "try:\n",
        "    # Split di training\n",
        "    cifar_train = datasets.CIFAR10(root=\"./data\", train=True,\n",
        "                                   transform=transform_cifar, download=True)\n",
        "    # Split di test\n",
        "    cifar_test  = datasets.CIFAR10(root=\"./data\", train=False,\n",
        "                                   transform=transform_cifar, download=True)\n",
        "except Exception as e:\n",
        "    # In caso di problemi di rete o permessi, fallback su FakeData per non bloccare il notebook\n",
        "    print(\"‚ö†Ô∏è Impossibile scaricare CIFAR-10. Fallback su FakeData. Errore:\", str(e))\n",
        "    cifar_train = datasets.FakeData(size=50000, image_size=(3,224,224),\n",
        "                                    num_classes=10, transform=transform_cifar)\n",
        "    cifar_test  = datasets.FakeData(size=10000, image_size=(3,224,224),\n",
        "                                    num_classes=10, transform=transform_cifar)\n",
        "\n",
        "# ----------------------------\n",
        "# 3Ô∏è‚É£ Creazione DataLoader\n",
        "# ----------------------------\n",
        "# batch_size=64 ‚Üí 64 immagini per batch\n",
        "# shuffle=True nel training ‚Üí mescola i dati a ogni epoca per migliorare la generalizzazione\n",
        "# pin_memory=True (solo su GPU): migliora le prestazioni delle copie CPU‚ÜíGPU\n",
        "cifar_train_loader = DataLoader(cifar_train, batch_size=64, shuffle=True,\n",
        "                                num_workers=2, pin_memory=True if device.type=='cuda' else False)\n",
        "\n",
        "# Per il test non serve shuffle (l'ordine non influisce)\n",
        "cifar_test_loader  = DataLoader(cifar_test,  batch_size=64, shuffle=False,\n",
        "                                num_workers=2, pin_memory=True if device.type=='cuda' else False)\n",
        "\n",
        "# ----------------------------\n",
        "# 4Ô∏è‚É£ Caricamento modello pre-addestrato\n",
        "# ----------------------------\n",
        "# Otteniamo i pesi predefiniti di ResNet18 (pretrained su ImageNet)\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "\n",
        "# Carichiamo la rete con quei pesi\n",
        "backbone = resnet18(weights=weights)\n",
        "\n",
        "# ----------------------------\n",
        "# 5Ô∏è‚É£ Congelamento dei layer di feature extraction\n",
        "# ----------------------------\n",
        "# In questo modo NON aggiorniamo i pesi delle convoluzioni gi√† apprese su ImageNet\n",
        "# (evitiamo di distruggere conoscenze generali come rilevatori di bordi, texture, forme)\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# ----------------------------\n",
        "# 6Ô∏è‚É£ Sostituzione della testa di classificazione\n",
        "# ----------------------------\n",
        "# ResNet18 pre-addestrata ha un layer finale (fc) con 1000 output (per ImageNet)\n",
        "# Lo sostituiamo con un layer fully connected per 10 classi (CIFAR-10)\n",
        "num_feats = backbone.fc.in_features   # Numero di feature in input alla fc (512 per ResNet18)\n",
        "backbone.fc = nn.Linear(num_feats, 10)\n",
        "\n",
        "# Spostiamo tutto il modello sul device (CPU o GPU)\n",
        "backbone = backbone.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# 7Ô∏è‚É£ Ottimizzatore e loss function\n",
        "# ----------------------------\n",
        "# Addestriamo SOLO la nuova testa (gli altri layer restano congelati)\n",
        "optimizer = optim.Adam(backbone.fc.parameters(), lr=1e-3)\n",
        "# Loss per classificazione multi-classe\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ----------------------------\n",
        "# 8Ô∏è‚É£ Mini addestramento dimostrativo\n",
        "# ----------------------------\n",
        "# Solo 1 epoca per mostrare come funziona il transfer learning\n",
        "epoch = 1\n",
        "print(\"\\n[Transfer Learning] Demo veloce\")\n",
        "\n",
        "# Eseguiamo un‚Äôepoca di training sul train_loader\n",
        "tl, ta = train_one_epoch(backbone, cifar_train_loader, optimizer, criterion)\n",
        "# E valutiamo sul test_loader\n",
        "vl, va = evaluate(backbone, cifar_test_loader, criterion, phase=\"Test\")\n",
        "\n",
        "# Stampiamo le metriche di performance\n",
        "print(f\"loss_train={tl:.4f}  acc_train={ta:.4f} | loss_test={vl:.4f}  acc_test={va:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classi di CIFAR-10\n",
        "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def predict_and_show(model, loader, n=5):\n",
        "    \"\"\"\n",
        "    Mostra n immagini del loader con la predizione del modello.\n",
        "    \"\"\"\n",
        "    model.eval()  # disattiva dropout e batchnorm in modalit√† training\n",
        "    xb, yb = next(iter(loader))          # prende il primo batch\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    with torch.no_grad():                # no gradient ‚Üí pi√π veloce, meno memoria\n",
        "        preds = model(xb).argmax(1)      # indice della classe con logit pi√π alto\n",
        "\n",
        "    xb = xb.cpu().permute(0, 2, 3, 1)    # da (N,C,H,W) ‚Üí (N,H,W,C) per Matplotlib\n",
        "    xb = xb * torch.tensor((0.229, 0.224, 0.225)) + torch.tensor((0.485, 0.456, 0.406))\n",
        "    xb = torch.clamp(xb, 0, 1)           # de-normalizzazione per visualizzare correttamente\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i in range(n):\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(xb[i])\n",
        "        plt.title(f\"True: {cifar_classes[yb[i]]}\\nPred: {cifar_classes[preds[i]]}\",\n",
        "                  fontsize=10)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# üîç Eseguiamo la funzione di predizione\n",
        "predict_and_show(backbone, cifar_test_loader, n=5)\n"
      ],
      "metadata": {
        "id": "wzNg4cHfulwi"
      },
      "id": "wzNg4cHfulwi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00d4c2cb",
      "metadata": {
        "id": "00d4c2cb"
      },
      "source": [
        "\n",
        "## 7. Forme (shape) e trucchi di debug\n",
        "\n",
        "**Dimensione di uscita di `Conv2d`** (per una sola dimensione):  \n",
        "\\[ \\text{out} = \\left\\lfloor \\frac{\\text{in} + 2\\cdot\\text{padding} - \\text{kernel}}{\\text{stride}} \\right\\rfloor + 1 \\]\n",
        "\n",
        "Con **Pooling** 2√ó2 e stride 2 spesso **dimezzi** H e W.  \n",
        "Con **Flatten** converti `(N, C, H, W)` in `(N, C¬∑H¬∑W)` prima di un `nn.Linear`.\n",
        "\n",
        "### Suggerimenti pratici\n",
        "- Stampa le **shape** in `forward` quando qualcosa non torna.  \n",
        "- Parti **semplice**; aggiungi funzioni (dropout/BN) una alla volta.  \n",
        "- Controlla la **normalizzazione** degli input; i modelli pre-addestrati si aspettano medie/std specifiche.  \n",
        "- Valida sempre su un **holdout** e monitora **loss e accuratezza**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7727c6c",
      "metadata": {
        "id": "c7727c6c"
      },
      "source": [
        "\n",
        "## 8. Esercizi\n",
        "\n",
        "1. **Gioca coi kernel:** modifica il kernel nella demo di convoluzione (blur, sharpen, ecc.) e osserva le differenze.  \n",
        "2. **CNN pi√π profonda:** aggiungi un terzo layer conv; adegua l'input del `Linear`.  \n",
        "3. **Sweep di regolarizzazione:** prova diversi `p` del Dropout e valori di `weight_decay`.  \n",
        "4. **BatchNorm on/off:** confronta le curve di apprendimento con e senza BN.  \n",
        "5. **Scheduler LR:** sostituisci `StepLR` con `OneCycleLR` o `CosineAnnealingLR`.  \n",
        "6. **Early stopping:** implementa uno stop precoce sulla loss di validazione.  \n",
        "7. **Confusion matrix:** calcola e visualizza una matrice di confusione per MNIST.  \n",
        "8. **Transfer learning:** sblocca l‚Äôultimo blocco residuo in ResNet18 e fai fine-tuning.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}